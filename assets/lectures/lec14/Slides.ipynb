{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:45.146161Z",
     "start_time": "2018-03-01T19:14:44.481187Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from autograd import elementwise_grad as egrad\n",
    "# from autograd import grad as grad\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "from plotly import tools\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\n",
    "    '<script>'\n",
    "        'var waitForPlotly = setInterval( function() {'\n",
    "            'if( typeof(window.Plotly) !== \"undefined\" ){'\n",
    "                'MathJax.Hub.Config({ SVG: { font: \"STIX-Web\" }, displayAlign: \"center\" });'\n",
    "                'MathJax.Hub.Queue([\"setRenderer\", MathJax.Hub, \"SVG\"]);'\n",
    "                'clearInterval(waitForPlotly);'\n",
    "            '}}, 250 );'\n",
    "    '</script>'\n",
    "))\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "    <style>\n",
    "        .output {\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            text-align: center;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\"))\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline = False, \n",
    "                   world_readable = True, \n",
    "                   theme = 'ggplot') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:45.153220Z",
     "start_time": "2018-03-01T19:14:45.147830Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "TOGGLER = HTML('''<script>\n",
    "                    code_show=true; \n",
    "                    function code_toggle() {\n",
    "                    if (code_show){\n",
    "                      $('div.input').hide();\n",
    "                    } else {\n",
    "                      $('div.input').show();\n",
    "                    }\n",
    "                    code_show = !code_show\n",
    "                    } \n",
    "                    $( document ).ready(code_toggle);\n",
    "                  </script>\n",
    "<a href=\"javascript:code_toggle()\">Toggle raw code</a>''')\n",
    "\n",
    "TOGGLER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture 14:\n",
    "## Intro to Optimization for Data Science\n",
    "\n",
    "\n",
    "<br><br>\n",
    "#### Slides by:\n",
    "### Jake A. Soloff \n",
    "#### <jake_soloff@berkeley.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scipy has Optimization Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    minimize(loss, x0 = ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So... let's call it a day?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview \n",
    "\n",
    "<br>\n",
    "\n",
    "- Loss Minimization Framework\n",
    "- Application to Recommender Systems\n",
    "- Optimization Problems\n",
    "- Gradient Descent (GD)\n",
    "- GD for Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation by Loss Minimization - a Recipe \n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- *Design a model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- *Choose loss function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- *Minimize loss function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example - Summarize data with a single number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Observe*: numeric data $X_1,\\dots, X_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- *Design a model*: a single numeric summary $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- *Choose loss function*: $\\mathbb{L}_2$-loss, or sum-of-square deviations $\\ell(\\theta) = \\sum_{i=1}^n (X_i-\\theta)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- *Minimize loss function*: $0 = \\left.\\frac{\\text{d}\\ell}{\\text{d}\\theta}\\right|_{\\theta = \\widehat\\theta} = \\sum_{i=1}^n2(\\widehat\\theta - X_i)~$ $\\Longrightarrow \\widehat\\theta = \\frac{1}{n}\\sum_{i=1}^n X_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example - Recommender System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- a company sells **items**, collects feedback (usually called a **rating**) from customers (called **users**).\n",
    "- rather than listing the most popular items, the company wants to make **personalized** recommendations\n",
    "- the cost of errors varies across applications\n",
    "<img src=\"amazonrecs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"nytrecs.png\", width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- *Observe*: a \"rating\" $r_{ui}$ by user $u$ on item $i$. A form of feedback:\n",
    "    - $5$-star rating\n",
    "    - Purchase\n",
    "    - Dwell time \n",
    "    - Not clicking \"next\"\n",
    "    \n",
    "    \n",
    "- User $u$ does not rate every item $i$ \n",
    "    - some ratings are *unobserved*\n",
    "    \n",
    "    \n",
    "- We (may) observe *features* $x_i = (x_{i1},x_{i2})$ about each item $i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- *Design a model*: a vector of \"weights\" $\\theta = (\\theta_1,\\theta_2)$ such that\n",
    "$$r_{ui} \\approx x_i\\cdot \\theta = x_{i1}\\theta_1 + x_{i2}\\theta_2.$$\n",
    "\n",
    "- **Notice** the right-hand-side above does not depend on $u$, so this model is not personalized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- *Design a personalized model*: a \"preference\" vector $y_u = (y_{u1},y_{u2})$ for user $u$ such that\n",
    "$$r_{ui} \\approx x_i\\cdot y_u$$\n",
    "(different weights for each user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- *Choose loss function*: $\\mathbb{L}_2$-loss, or sum-of-square deviations $\\ell(y_1,\\dots,y_u) = \\sum_{u,i} (r_{ui}-x_i\\cdot y_u)^2$.\n",
    "    - notice the above loss function only sums over pairs $(u,i)$ for which we observe $r_{ui}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- *Minimize loss function* (but <font color=blue>how?</font>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"volinsky.png\", width=50%>\n",
    "\n",
    "(Koren, Bell & Volinsky 2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Function Minimization\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "An *unconstrained optimization problem* is denoted\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{x}\\,f(x),\n",
    "\\end{align}\n",
    "where\n",
    "- $x$ is a collection of *decision variables*.\n",
    "- $f(x)$ is the associated *cost*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our context, we have a loss function $\\ell(\\theta)$ and want to solve\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{\\theta}\\,\\ell(\\theta),\n",
    "\\end{align}\n",
    "\n",
    "which is to say we want to find $\\widehat\\theta$ such that $\\ell\\left(\\widehat\\theta\\right) \\le \\ell(\\theta)$ for all $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Minimizing Analytically\n",
    "Say $\\ell(\\theta) = (\\theta - 1)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:45.157806Z",
     "start_time": "2018-03-01T19:14:45.154679Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TOGGLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:45.580493Z",
     "start_time": "2018-03-01T19:14:45.159292Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "x = np.arange(-5, 5, .01)\n",
    "plt.plot(x, (x-1)**2, 'b')\n",
    "plt.plot(1, 0.0, '|', color = 'r', \n",
    "         markersize = 20, markeredgewidth = 4)\n",
    "plt.xlabel('$\\\\theta$')\n",
    "plt.ylabel('$\\ell(\\\\theta)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- look at the graph and see $\\widehat\\theta\\approx 1$.\n",
    "- notice $\\ell(\\theta) \\ge 0$ and $\\ell(1) = 0$.\n",
    "- set derivative to zero. $0 = \\left.\\frac{\\text{d}\\ell}{\\text{d}\\theta}\\right|_{\\theta = \\widehat\\theta} = 2(\\widehat\\theta - 1) \\text{ so } \\widehat\\theta = 1.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Minimizing Analytically (cont'd)\n",
    "\n",
    "Now $\\ell(\\theta) = (\\theta - 1)^2 + (\\theta+2)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:45.586032Z",
     "start_time": "2018-03-01T19:14:45.582600Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TOGGLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:45.721953Z",
     "start_time": "2018-03-01T19:14:45.587662Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "theta = np.arange(-4, 4, .01)\n",
    "a, b = -2, 1\n",
    "plt.plot(theta, (theta-a)**2, 'k--')\n",
    "plt.plot(theta, (theta-b)**2, 'k--')\n",
    "plt.plot(theta, (theta-a)**2+(theta-b)**2, 'b')\n",
    "plt.plot([a,b], [0]*2, '|', color = 'k', \n",
    "         markersize = 20, markeredgewidth = 4)\n",
    "plt.plot(0.5*(a+b), 4.5, '|', color = 'r',\n",
    "        markersize = 20, markeredgewidth = 4)\n",
    "plt.xlabel('$\\\\theta$')\n",
    "plt.ylabel('$\\ell(\\\\theta)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Minimizing Analytically (cont'd)\n",
    "\n",
    "Now $\\ell(\\theta) = \\theta^2(\\theta^2-4)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:45.727392Z",
     "start_time": "2018-03-01T19:14:45.723740Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TOGGLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:45.847995Z",
     "start_time": "2018-03-01T19:14:45.729163Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "x = np.arange(-2.5, 2.5, .01)\n",
    "plt.plot(x, x**2*(x**2-4)**2, 'b')\n",
    "plt.plot([-2,-np.sqrt(4/3),0,np.sqrt(4/3),2], \n",
    "         [0, 256/27, 0, 256/27, 0], '|', color='r',\n",
    "         markersize = 20, markeredgewidth = 4)\n",
    "plt.xlabel('$\\\\theta$')\n",
    "plt.ylabel('$\\ell(\\\\theta)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remember that $\\frac{\\text{d}\\ell}{\\text{d}\\theta} = 0$ does *not* imply $\\theta$ is a minimizer!\n",
    "\n",
    "(might try $2^{\\text{nd}}$ derivative test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convexity Revisited\n",
    "\n",
    "- Assuming convexity, $\\ell'(\\eta) = 0$ does tell us that $\\eta$ is a *global minimizer*, as we will now show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Proof by Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:45.853329Z",
     "start_time": "2018-03-01T19:14:45.849858Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TOGGLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:46.114768Z",
     "start_time": "2018-03-01T19:14:45.855027Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "xx = np.linspace(-4,4,50)\n",
    "yy = 3*(xx+1)**2 + 2\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(xx,yy)\n",
    "plt.plot(np.linspace(-2,1,50), 3*np.linspace(-2,1,50) + 11,'r-')\n",
    "plt.plot([-2,1],[5,14],'kx',markersize=20,markeredgewidth=2)\n",
    "plt.ylim([-20,80])\n",
    "plt.title(\"zeroth order condition for convexity\") #  $f(\\lambda x+(1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)$\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xx,yy)\n",
    "plt.plot(xx,12*xx+2,'r-')\n",
    "plt.plot(1,14,'kx',markersize=20,markeredgewidth=2)\n",
    "plt.ylim([-20,80])\n",
    "plt.title(\"first order condition for convexity\") # $f(y) \\geq f(x) + (y-x)\\\\frac{df}{dx}(x)$\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Direct Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Recall convextiy for the loss $\\ell$ means for all $0\\le\\lambda\\le 1$\n",
    "$$\\ell(\\lambda\\theta + (1-\\lambda)\\eta) \\le \\lambda\\ell(\\theta) + (1-\\lambda)\\ell(\\eta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Group terms with $\\lambda$ in front:\n",
    "\n",
    "$$\\ell(\\eta +\\lambda(\\theta - \\eta)) \\le \\ell(\\eta) +  \\lambda(\\ell(\\theta) - \\ell(\\eta)).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Subtract $\\ell(\\eta)$ and divide by $\\lambda$:\n",
    "\n",
    "$$\\frac{\\ell(\\eta +\\lambda(\\theta - \\eta)) - \\ell(\\eta)}{\\lambda}\n",
    " \\le \\ell(\\theta) - \\ell(\\eta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Make ratio look like a derivative:\n",
    "\n",
    "$$\\underbrace{\\frac{\\ell(\\eta +\\lambda(\\theta - \\eta)) - \\ell(\\eta)}{\\lambda(\\theta - \\eta)}}_{\\rightarrow \\ell'(\\eta)}\n",
    "(\\theta - \\eta) \\le (\\ell(\\theta) - \\ell(\\eta)).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Hence (if $\\ell$ is convex and differentiable)\n",
    "\n",
    "$$\\ell(\\eta) + \\ell'(\\eta)(\\theta - \\eta) \\le \\ell(\\theta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $\\ell(\\theta)$ is convex, it lies above its tangent approximation **everywhere**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why convexity? Recap\n",
    "- We showed if $\\ell$ is convex and differentiable, then for all $\\eta,\\theta$,\n",
    "$$\\ell(\\eta) + \\ell'(\\eta)(\\theta - \\eta) \\le \\ell(\\theta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In particular, if $\\ell'(\\eta) = 0$ (i.e. $\\eta$ is a *critical point*), then\n",
    "$$\\ell(\\eta) \\le \\ell(\\theta) \\text{ for all } \\theta.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Checking that $\\ell'(\\eta) = 0$ tells us that $\\eta$ is a *global minimizer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What about when $\\theta$ is not a single number?\n",
    "- **Example.** $\\theta = (\\theta_1,\\theta_2)$ is a *vector* and $\\ell(\\theta) = \\theta_1^2 + 4\\theta_2^2$.\n",
    "- The gradient is $\\nabla\\ell(\\theta) = \\left(\\frac{\\partial \\ell}{\\partial \\theta_1}, \\frac{\\partial \\ell}{\\partial \\theta_2}\\right) = (2\\theta_1, 8\\theta_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:46.122391Z",
     "start_time": "2018-03-01T19:14:46.117481Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TOGGLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:46.388009Z",
     "start_time": "2018-03-01T19:14:46.124722Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "a, b = -3, 3\n",
    "x = np.arange(a, b, .05)\n",
    "y = np.arange(a, b, .05)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "z = X**2 + 4*Y**2\n",
    "\n",
    "p2 = Surface(name = \"Loss Surface\",\n",
    "             x = x, \n",
    "             y = y, \n",
    "             z = z, \n",
    "             colorscale = 'Viridis',\n",
    "             reversescale = True,\n",
    "             showscale = False)\n",
    "\n",
    "paraboloid = Contour(name = \"Loss Contour\",\n",
    "                     x = x, \n",
    "                     y = y, \n",
    "                     z = z, \n",
    "                     colorscale = 'Viridis',\n",
    "                     reversescale = True, \n",
    "                     autocontour = True,\n",
    "                     xaxis = 'x2',\n",
    "                     yaxis = 'y2')\n",
    "\n",
    "lay = Layout(xaxis = {'range' : [a, b],\n",
    "                      'title' : '$\\\\theta_1$'}, \n",
    "             yaxis = {'range' : [a, b],\n",
    "                      'title' : '$\\\\theta_2$'},\n",
    "             scene = {\"domain\": {'x': [0, 0.48], \n",
    "                                 'y': [0, 1]},\n",
    "                      'xaxis' : {'title' : 'θ1'},\n",
    "                      'yaxis' : {'title' : 'θ2'},\n",
    "                      'zaxis' : {'title' : 'ℓ(θ1,θ2)'}},\n",
    "             xaxis2= {'domain' : [.52, 1],\n",
    "                      'range' : [a, b],\n",
    "                      'title' : '$\\\\theta_1$'},\n",
    "             yaxis2= {'anchor' : 'x2',\n",
    "                      'range' : [a, b],\n",
    "                      'title' : '$\\\\theta_2$'},\n",
    "             autosize = False,\n",
    "             width = 1000,\n",
    "             height = 600)\n",
    "\n",
    "iplot(Figure(data = Data([p2, paraboloid]), layout = lay))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Zero Gradient Condition\n",
    "- If $\\theta$ is a single variable, $\\ell'(\\theta) = 0$ means the loss is *locally flat* around $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $\\theta = (\\theta_1,\\theta_2)$, the *partial derivatives* $\\frac{\\partial \\ell}{\\partial \\theta_j}$ represent the derivative along one coordinate with the other held fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Hence if the gradient $\\nabla \\ell(\\theta) = \\left(\\frac{\\partial \\ell}{\\partial \\theta_1}, \\frac{\\partial \\ell}{\\partial \\theta_2}\\right) = 0$, the function is *locally flat* around $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $\\ell$ is convex and $\\nabla\\ell(\\eta) = 0$, then $\\eta$ is a global minimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gradient: steepest ascent, perpendicular to level curves (contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:46.395361Z",
     "start_time": "2018-03-01T19:14:46.391251Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TOGGLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:46.780670Z",
     "start_time": "2018-03-01T19:14:46.396818Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "a, b = -3, 3\n",
    "x = np.arange(a, b, .05)\n",
    "y = np.arange(a, b, .05)\n",
    "\n",
    "trace1 = Contour(name = \"Loss Contour\",\n",
    "                 x = x, \n",
    "                 y = y, \n",
    "                 z = z, \n",
    "                 colorscale = 'Viridis',\n",
    "                 reversescale = True, \n",
    "                 showscale = False,\n",
    "                 autocontour = True,)\n",
    "\n",
    "lay = Layout(xaxis = {'range' : [a, b],\n",
    "                      'title' : '$\\\\theta_1$'}, \n",
    "             yaxis = {'range' : [a, b],\n",
    "                      'title' : '$\\\\theta_2$'},\n",
    "             width = 600,\n",
    "             height = 600)\n",
    "\n",
    "xx = np.linspace(a, b, 7)\n",
    "yy = np.linspace(a, b, 7)\n",
    "X, Y = np.meshgrid(xx, yy)\n",
    "\n",
    "trace2 = ff.create_quiver(X, Y, -2*X, -8*Y,\n",
    "                           scale = .025, \n",
    "                           arrow_scale = .15,)['data'][0]\n",
    "trace2['name'] = \"Negative Gradient\"\n",
    "trace2['marker']['color'] = 'black'\n",
    "trace2['marker']['line']['width'] = 8\n",
    "trace2['marker']['size'] = 8\n",
    "\n",
    "iplot(Figure(data = Data([trace1, trace2]), layout = lay))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent Intuition\n",
    "\n",
    "- Iterative optimization algorithm: given a candidate minimizer $\\theta^{(t)}$ at step $t$, want an improved version $\\theta^{(t+1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient moves along direction of steepest ascent, so $-\\nabla\\ell(\\theta)$ moves along direction of steepest *descent*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So to get from $\\theta^{(t)}$ to $\\theta^{(t+1)}$, we might as well follow the gradient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:46.786546Z",
     "start_time": "2018-03-01T19:14:46.782865Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TOGGLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:47.046622Z",
     "start_time": "2018-03-01T19:14:46.788685Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(-4,4,50)\n",
    "f = lambda x: 3*(x+1)**2 + 2\n",
    "df = lambda x: 6*(x+1)\n",
    "yy = f(xx)\n",
    "\n",
    "alpha = .2\n",
    "\n",
    "x0 = 1\n",
    "x1 = x0 - alpha*df(x0)\n",
    "x2 = x1 - alpha*df(x1)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(xx,yy)\n",
    "plt.plot(xx,12*xx+2,'r-')\n",
    "\n",
    "plt.plot(x0,-19,'|k', \n",
    "         markersize = 20, markeredgewidth = 4)\n",
    "plt.plot(x0,f(x0),'.k', \n",
    "         markersize = 15, markeredgewidth = 4)\n",
    "plt.annotate('$\\\\theta^{(0)}$', [x0 - .1, -17])\n",
    "plt.arrow(x0, -19, -alpha*df(x0), 0, \n",
    "          width = .5, head_length = 0.5, length_includes_head = True)\n",
    "plt.annotate('$-0.2\\ell\\'(\\\\theta^{(0)})$', [x1-1, -17])\n",
    "\n",
    "plt.axvline(x=-1, ls='--')\n",
    "plt.ylim([-20,80])\n",
    "plt.title('$\\\\ell(\\\\theta) = 3(\\\\theta+1)^2+2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:47.259578Z",
     "start_time": "2018-03-01T19:14:47.048466Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(xx,yy)\n",
    "plt.plot(xx,12*xx+2,'r-')\n",
    "\n",
    "plt.plot(x1,-19,'|k', \n",
    "         markersize = 20, markeredgewidth = 4)\n",
    "plt.plot(x1,f(x1),'.k', \n",
    "         markersize = 15, markeredgewidth = 4)\n",
    "plt.annotate('$\\\\theta^{(1)}$', [x1 - .1, -17])\n",
    "plt.arrow(x1, -19, -alpha*df(x1), 0, \n",
    "          width = .25, head_length = 0.25, length_includes_head = True)\n",
    "plt.annotate('$-0.2\\ell\\'(\\\\theta^{(1)})$', [x1+1, -17])\n",
    "\n",
    "plt.axvline(x = -1, ls='--')\n",
    "plt.ylim([-20, 80])\n",
    "plt.title('$\\\\ell(\\\\theta) = 3(\\\\theta+1)^2+2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:47.440462Z",
     "start_time": "2018-03-01T19:14:47.261425Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(xx,yy)\n",
    "plt.plot(xx,12*xx+2,'r-')\n",
    "\n",
    "plt.plot(x2,-19,'|k', \n",
    "         markersize = 20, markeredgewidth = 4)\n",
    "plt.plot(x2,f(x2),'.k', \n",
    "         markersize = 15, markeredgewidth = 4)\n",
    "plt.annotate('$\\\\theta^{(2)}$', [x2 - .1, -17])\n",
    "plt.arrow(x2, -19, -alpha*df(x2), 0, \n",
    "          width = .05, head_length = 0.05, length_includes_head = True)\n",
    "\n",
    "plt.axvline(x = -1, ls='--')\n",
    "plt.ylim([-20, 80])\n",
    "plt.title('$\\\\ell(\\\\theta) = 3(\\\\theta+1)^2+2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "- Initialize value $\\theta^{(0)}$ (zeros, random guess, or output of another method)\n",
    "\n",
    "- For $t$ from $0$ until convergence, update\n",
    "\n",
    "$$\\theta^{(t+1)} \\leftarrow \\theta^{(t)} - \\alpha\\nabla \\ell(\\theta^{(t)})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\alpha$ is called the *learning rate* and need not be constant (i.e. use $\\alpha_t$).\n",
    "- For sufficiently small $\\alpha$ we will have $\\ell(\\theta^{(t+1)}) \\le \\ell(\\theta^{(t)})$.\n",
    "- We might stop once $\\nabla\\ell(\\theta^{(t)})$ is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:47.446868Z",
     "start_time": "2018-03-01T19:14:47.442879Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "TOGGLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:47.456905Z",
     "start_time": "2018-03-01T19:14:47.449153Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(theta0, grad, alpha, tol = 1e-4, max_iter = 1e5):\n",
    "    theta_path = [theta0]\n",
    "    i = 0\n",
    "    while(np.linalg.norm(grad(theta_path[-1])) > tol) & (i < max_iter):\n",
    "        i += 1\n",
    "        theta_t = theta_path[-1]\n",
    "        theta_path.append(theta_t - alpha*grad(theta_t))\n",
    "    return np.array(theta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:47.608172Z",
     "start_time": "2018-03-01T19:14:47.459372Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "a, b = -3, 3\n",
    "x = np.arange(a, b, .05)\n",
    "y = np.arange(a, b, .05)\n",
    "\n",
    "trace1 = Contour(name = \"Loss Contour\",\n",
    "                 x = x, \n",
    "                 y = y, \n",
    "                 z = z, \n",
    "                 colorscale = 'Viridis',\n",
    "                 reversescale = True, \n",
    "                 showscale = False,\n",
    "                 autocontour = True,)\n",
    "\n",
    "lay = Layout(xaxis = {'range' : [a, b],\n",
    "                      'title' : '$\\\\theta_1$'}, \n",
    "             yaxis = {'range' : [a, b],\n",
    "                      'title' : '$\\\\theta_2$'},\n",
    "             width = 600,\n",
    "             height = 600)\n",
    "\n",
    "def trace2(theta0 = np.array([2.3, 2.5]), alpha = 0.01):\n",
    "    theta_path = gradient_descent(theta0, \n",
    "                                  lambda theta: np.array([2*theta[0], 8*theta[1]]), \n",
    "                                  alpha,\n",
    "                                  tol = 1e-2)\n",
    "    return Scatter(name = \"Theta Path\", \n",
    "                     x = theta_path[:,0], \n",
    "                     y = theta_path[:,1],\n",
    "                     mode = \"lines+markers\")\n",
    "\n",
    "title = lambda a : '$\\\\alpha = ' + str(a) + '$'\n",
    "iplot(Figure(data = [trace1, trace2(alpha = 0.01)], layout = {**lay, **{'title': title(.01)}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:47.733469Z",
     "start_time": "2018-03-01T19:14:47.610026Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "iplot(Figure(data = [trace1, trace2(alpha = 0.1)], layout = {**lay, **{'title': title(.1)}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:47.859539Z",
     "start_time": "2018-03-01T19:14:47.735128Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "iplot(Figure(data = [trace1, trace2(alpha = 0.23)], layout = {**lay, **{'title': title(.23)}}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic Differentiation \n",
    "\n",
    "\n",
    "        import autograd.numpy as np\n",
    "        from autograd import grad\n",
    "        \n",
    "        def loss(theta):\n",
    "            ...\n",
    "        \n",
    "        grad(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example - Recommender System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:14:47.864698Z",
     "start_time": "2018-03-01T19:14:47.861557Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TOGGLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T19:14:44.600Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T19:14:44.603Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from surprise import Dataset, SVD\n",
    "\n",
    "# Load MovieLens dataset\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Retrieve training set.\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Fit probabilistic matrix factorization (PMF) model\n",
    "algo = SVD(n_factors = 2, \n",
    "           n_epochs = 100, \n",
    "           #biased = False,\n",
    "           random_state = 1234567891)\n",
    "algo.fit(trainset) \n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T19:14:44.605Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "rnames = ['uid', 'iid', 'rating', 'time']\n",
    "ratings = pd.read_csv('./ml-100k/u.data', \n",
    "                      sep = '\\t', \n",
    "                      header = None, \n",
    "                      names = rnames)\\\n",
    "                     .drop(columns = ['time'])\n",
    "    \n",
    "inames = ['iid', 'title', 'release date', 'video release date', 'IMDb URL', \n",
    "          'unknown', 'Action', 'Adventure', 'Animation', 'Children\\'s', \n",
    "          'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "          'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', \n",
    "          'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "items = pd.read_csv('./ml-100k/u.item', \n",
    "                      sep = '|', \n",
    "                      header = None, \n",
    "                      names = inames,\n",
    "                      encoding = 'iso8859_2')\\\n",
    "                    .drop(columns = ['release date', 'video release date', \n",
    "                                     'IMDb URL', 'unknown'])\n",
    "\n",
    "item_names = items.iloc[:,[0,1]].set_index('iid')\n",
    "\n",
    "tot_rating = ratings.groupby('iid').sum().drop(columns = ['uid'])\n",
    "avg_rating = ratings.groupby('iid').mean().drop(columns = ['uid'])\n",
    "top_titles = avg_rating[(tot_rating > 1300) & (avg_rating > 3.82)].dropna().join(item_names)\n",
    "inds = list(top_titles.index-1)\n",
    "top_titles.sort_values('rating', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this model, both user vectors $y_u$ and item vectors $x_i$ are unobserved:\n",
    "$$r_{ui} \\approx x_i\\cdot y_u$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T19:14:44.608Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (17, 17))\n",
    "plt.plot(algo.qi[inds, 0], algo.qi[inds, 1], 'k.')\n",
    "for i in inds:\n",
    "    title = item_names.loc[i+1,'title'][:-6]\n",
    "    if title.strip().endswith(', The'):\n",
    "        title = \"The \" + title[:-6]\n",
    "    plt.annotate(title, algo.qi[i])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## On Worrying Selectively\n",
    "\n",
    "<br>\n",
    "\n",
    "    \"Since all models are wrong the scientist \n",
    "    must be alert to what is importantly wrong. \n",
    "    It is inappropriate to be concerned about \n",
    "    mice when there are tigers abroad.\"\n",
    "      - George E. P. Box, Science and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "- Loss Minimization as a Recipe for Estimation \n",
    "- Concepts and Tools from Numerical Optimization\n",
    "- Application to Recommender Systems"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
