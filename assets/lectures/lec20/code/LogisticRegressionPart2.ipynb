{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=False)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=False, world_readable=True, theme='ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This is the notebook accompanies the lecture on Logistic Regression.\n",
    "\n",
    "Notebook created by [Joseph E. Gonzalez](https://eecs.berkeley.edu/~jegonzal) for DS100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data\n",
    "\n",
    "For this lecture we will use the famous Wisconsin Breast Cancer Dataset which we can obtain from [scikit learn](http://scikit-learn.org/stable/datasets/index.html#breast-cancer-wisconsin-diagnostic-database).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "data_dict = sklearn.datasets.load_breast_cancer()\n",
    "data = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "# Target data_dict['target'] = 0 is malignant 1 is benign\n",
    "data['malignant'] = (data_dict['target'] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a clear example of over-plotting.  We can improve the above plot by jittering the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_y = data['malignant'] + 0.1*np.random.rand(len(data['malignant'])) -0.05\n",
    "points = go.Scatter(x=data['mean radius'], y = jitter_y, mode=\"markers\", marker=dict(opacity=0.5))\n",
    "tumor_layout = dict(xaxis=dict(title=\"Mean Radius\"),yaxis=dict(title=\"Malignant\"))\n",
    "py.iplot(go.Figure(data=[points], layout=tumor_layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a better way to visualize the data is using stacked histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(ff.create_distplot([data.loc[~data['malignant'], 'mean radius'],\n",
    "                            data.loc[data['malignant'], 'mean radius']], \n",
    "                            group_labels=[\"Benign\",\"Malignant\"],\n",
    "                           bin_size=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always split your data into training and test groups.  \n",
    "\n",
    "## Preparing the data Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_tr, data_te = train_test_split(data, test_size=0.25, random_state=42)\n",
    "print(\"Training Data Size: \", len(data_tr))\n",
    "print(\"Test Data Size: \", len(data_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define $X$ and $Y$ as variables containing the training features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data_tr[['mean radius']].values\n",
    "Y = data_tr['malignant'].values.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "# Empirical Probability \n",
    "\n",
    "What does the empirical probability of being malignant for different sizes look like?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_slices = np.linspace(8, 24, 10)\n",
    "P_emp = np.array([\n",
    "    np.mean(Y[np.squeeze(np.abs(X - c)) < 2.0]) for c in X_slices\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as linear_model\n",
    "least_squares_model = linear_model.LinearRegression()\n",
    "least_squares_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Least Squares and Empirical Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_y = Y + 0.1*np.random.rand(len(Y)) - 0.05\n",
    "ind_mal = least_squares_model.predict(X) > 0.5\n",
    "X_plt = np.linspace(np.min(X), np.max(X), 1000)\n",
    "model_line = go.Scatter(name=\"Least Squares\",\n",
    "    x=X_plt, y=least_squares_model.predict(np.array([X_plt]).T), \n",
    "    mode=\"lines\", line=dict(color=\"orange\"))\n",
    "\n",
    "mal_points = go.Scatter(name=\"Classified as Malignant\", \n",
    "                    x=np.squeeze(X[ind_mal]), y = jitter_y[ind_mal], \n",
    "                    mode=\"markers\", marker=dict(opacity=0.5, color=\"red\"))\n",
    "ben_points = go.Scatter(name=\"Classified as Benign\", \n",
    "                    x=np.squeeze(X[~ind_mal]), y = jitter_y[~ind_mal], \n",
    "                    mode=\"markers\", marker=dict(opacity=0.5, color=\"blue\"))\n",
    "dec_boundary = (0.5 - least_squares_model.intercept_)/least_squares_model.coef_[0]\n",
    "dec_line = go.Scatter(name=\"Least Squares Decision Boundary\", \n",
    "                      x = [dec_boundary,dec_boundary], y=[-0.5,1.5], mode=\"lines\",\n",
    "                     line=dict(color=\"black\", dash=\"dot\"))\n",
    "\n",
    "p_emp_line = go.Scatter(name=\"Empirical Probabilities\",\n",
    "    x=X_slices, y=P_emp, \n",
    "    mode=\"lines\", line=dict(color=\"green\", width=8))\n",
    "py.iplot(go.Figure(data=[mal_points, ben_points, model_line, p_emp_line, dec_line], layout=tumor_layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Activation Function\n",
    "\n",
    "The **logistic function** (sometimes called the sigmoid function) has the above shape and is commonly used to transform a real number into a value that can be used to encode a probability.  The logistic function has some interesting mathematical properties but let's start with it's form:\n",
    "\n",
    "$$\\large\n",
    "\\sigma(t) = \\frac{1}{1 + e^{-t}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1. / (1. + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-5,5,50)\n",
    "sigmoid_line = go.Scatter(name=\"Logistic Function\",\n",
    "    x=t, y=sigmoid(t), mode=\"lines\")\n",
    "py.iplot([sigmoid_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Visualization\n",
    "\n",
    "The following plots $\\sigma(w t)$ for different $w$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-5,5,100)\n",
    "\n",
    "lines = []\n",
    "for w in np.linspace(0.5,10,20):\n",
    "    line = go.Scatter(name=str(w), x=t, y=sigmoid(w*t), mode=\"lines\", visible=False)\n",
    "    lines.append(line)\n",
    "\n",
    "lines[0].visible = True\n",
    "steps = []\n",
    "for i in range(len(lines)):\n",
    "    step = dict(\n",
    "        label = lines[i]['name'],\n",
    "        method = 'restyle',\n",
    "        args = ['visible', [False] * (len(lines)+1)],\n",
    "    )\n",
    "    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(active = 0, currentvalue = {\"prefix\": \"scale term: \"}, steps = steps)]\n",
    "\n",
    "# render the plot\n",
    "slider_layout = go.Layout(xaxis=dict(range=[t.min(), t.max()]), \n",
    "                   yaxis=dict(range=[-0.5 , 1.5]),\n",
    "                   showlegend=False,\n",
    "                   sliders=sliders)\n",
    "py.iplot(go.Figure(data = lines, layout=slider_layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots $\\sigma(5t + b)$ for different $v$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-5,5,100)\n",
    "\n",
    "lines = []\n",
    "for b in np.linspace(-10,10,20):\n",
    "    line = go.Scatter(name=str(w), x=t, y=sigmoid(5*t + b), mode=\"lines\", visible=False)\n",
    "    lines.append(line)\n",
    "\n",
    "lines[0].visible = True\n",
    "steps = []\n",
    "for i in range(len(lines)):\n",
    "    step = dict(\n",
    "        label = lines[i]['name'],\n",
    "        method = 'restyle',\n",
    "        args = ['visible', [False] * (len(lines)+1)],\n",
    "    )\n",
    "    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(active = 0, currentvalue = {\"prefix\": \"offset term: \"}, steps = steps)]\n",
    "\n",
    "# render the plot\n",
    "slider_layout = go.Layout(xaxis=dict(range=[t.min(), t.max()]), \n",
    "                   yaxis=dict(range=[-0.5 , 1.5]),\n",
    "                   showlegend=False,\n",
    "                   sliders=sliders)\n",
    "py.iplot(go.Figure(data = lines, layout=slider_layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Recall the algorithm consists of iteratively computing:\n",
    "\n",
    "$$\\large\n",
    "\\theta^{(t+1)} \\leftarrow \\theta^{(t)} - \\rho(t)\n",
    "\\left(\\nabla_\\theta \\mathbf{L}(\\theta) \\biggr\\rvert_{\\theta = \\theta^{(\\tau)}}\\right)\n",
    "$$\n",
    "\n",
    "where $\\rho(t)$ (sometimes called the learning rate) is typically:\n",
    "\n",
    "$$\n",
    "\\large \\rho(t) = \\frac{1}{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Logistic Regression \n",
    "\n",
    "The loss function for logistic regression is:\n",
    "\n",
    "\\begin{align}\n",
    "\\large\\textbf{L}(\\theta) &= \\large-\\frac{1}{n}\\sum_{i=1}^n y_i \\log\\left(\\sigma(\\phi(x_i)^T \\theta) \\right) +\n",
    "(1-y_i) \\log\\left(1 - \\sigma(\\phi(x_i)^T\\theta)\\right)\\\\\n",
    "& \\large = -\\frac{1}{n}\\sum_{i=1}^n  y_i \\phi(x_i)^T \\theta +\n",
    "\\log\\left(\\sigma(-\\phi(x_i)^T\\theta) \\right)\n",
    "\\end{align}\n",
    "\n",
    "The gradient of the loss is given by:\n",
    "$$\\large\n",
    "\\nabla_\\theta \\textbf{L}(\\theta)  = \\frac{1}{n}\\sum_{i=1}^n \n",
    " \\left(\\sigma\\left(\\phi(x_i)^T \\theta\\right) - y_i \\right) \\phi(x_i)\n",
    "$$\n",
    "\n",
    "\n",
    "The logistic regression update equation is then:\n",
    "$$\\large\n",
    "\\theta^{(t+1)} \\leftarrow \\theta^{(t)} - \\frac{1}{t}\n",
    "\\left(\\frac{1}{n}\\sum_{i=1}^n \n",
    " \\left(\\sigma\\left(\\phi(x_i)^T \\theta\\right) - y_i \\right) \\phi(x_i)\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, theta0, gradient_function, max_iter = 1000000,  \n",
    "                     epsilon=0.0001):\n",
    "    theta = theta0\n",
    "    for t in range(1, max_iter):\n",
    "        rho = 1./t\n",
    "        grad = gradient_function(theta, X, Y)\n",
    "        theta = theta - rho * grad\n",
    "        # Track Convergence\n",
    "        if np.linalg.norm(grad) < epsilon:\n",
    "            return theta\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence: When to stop iterating\n",
    "\n",
    "The gradient descent algorithm is typically run until one of the following conditions is met:\n",
    "\n",
    "1. $\\theta$ stops changing (by more than epsilon)\n",
    "1. a maximum number of iterations are run (i.e., user gets impatient)\n",
    "1. the gradient of the objective is zero (this is always a stopping condition, why?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining $\\nabla_\\theta  \\textbf{L}(\\theta) $\n",
    "\n",
    "In the following we implement the gradient function for a dataset X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradL(theta, X, Y):\n",
    "    return (X.T @ (sigmoid(X @ theta) - Y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we standardize this dataset for numerical stability.  Note that due to the exponents extreme values can be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardizer = StandardScaler()\n",
    "standardizer.fit(X)\n",
    "Xst = standardizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add a constant (bias or offset) term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Phi = np.hstack([Xst,np.ones([len(Xst),1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the batch gradient descent solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_batch = gradient_descent(Phi, Y, np.ones(2), gradL)\n",
    "theta_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with SK-Learn\n",
    "\n",
    "In practice you will again use libraries with optimized implementations of these algorithms.  However it is worth knowing how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_basic = LogisticRegression(fit_intercept=False, C=100.00)\n",
    "# 1) We already added an intercept to phi\n",
    "# 2) C = 1/lambda the inverse regularization parameter. \n",
    "#    by default SK learn adds regularization (we made it small)\n",
    "lr_basic.fit(Phi, Y)\n",
    "lr_basic.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is slightly different because the SKLearn logistic regression model automatically adds regularization.  This ensures a more stable solution and enables more advanced optimization algorithms to perform reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the solution\n",
    "\n",
    "In the following we plot the logistic regression probability model and the corresponding decision boundary.  To compute the decision boundary recall that from the model definition that $\\textbf{P}(Y = 1 | X) = \\sigma(x^T \\theta)$ and therefore the probability that $\\textbf{P}(Y = 1 | X) > 0.5$ occurs when $\\sigma(x^T \\theta) > 0.5$ and therefore $x^T \\theta > \\sigma^{-1}(0.5) = 0$.  Note however that we standardized the features in $X$ to construct $\\Phi$ and therefore after solving for $X$ at the decision boundary we will need to apply the inverse standardization (multiply by the standard deviation and add back the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_plt = np.hstack([\n",
    "    standardizer.transform(np.array([X_plt]).T), \n",
    "    np.ones((len(X_plt),1))])\n",
    "\n",
    "# The logistic regression decision boundary is when the underlying linear\n",
    "# model passes through zero\n",
    "lr_dec_boundary = (0.0 - theta_batch[1])/theta_batch[0]\n",
    "lr_dec_boundary = standardizer.inverse_transform([lr_dec_boundary])[0]\n",
    "lr_dec_line = go.Scatter(name=\"Logistic Reg. Decision Boundary\", \n",
    "                      x = [lr_dec_boundary,lr_dec_boundary], y=[-0.5,1.5], mode=\"lines\",\n",
    "                     line=dict(color=\"red\", dash=\"dot\"))\n",
    "\n",
    "lr_line = go.Scatter(name=\"Gradeint Descent Logistic Regression\",\n",
    "    x=X_plt, y=sigmoid(Phi_plt @ theta_batch), \n",
    "    mode=\"lines\", line=dict(color=\"orange\", width=4))\n",
    "py.iplot(go.Figure(data=[points, lr_line, dec_line, lr_dec_line], layout=tumor_layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the decision boundaries to our earlier histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = ff.create_distplot([\n",
    "    data[~data['malignant']]['mean radius'],\n",
    "    data[data['malignant']]['mean radius']\n",
    "], group_labels=['benign', 'malignant'],  bin_size=0.5)\n",
    "plt.data.append(go.Scatter(name=\"Logistic Reg. Decision Boundary\", \n",
    "                      x = [lr_dec_boundary,lr_dec_boundary], y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"red\", dash=\"dot\")))\n",
    "plt.data.append(go.Scatter(name=\"Least Squares Decision Boundary\", \n",
    "                      x = [dec_boundary,dec_boundary], y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"black\", dash=\"dot\")))\n",
    "py.iplot(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison in Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "print(\"Least Squares Training Prediction Error:\", \n",
    "      zero_one_loss(Y, least_squares_model.predict(X) > 0.5))\n",
    "print(\"Logistic Regression Prediction Error:\", \n",
    "      zero_one_loss(Y, sigmoid(Phi @ theta_batch) > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the loss surface look like?\n",
    "\n",
    "Let's examine the loss function surface and our solution.  Recall that the **loss function** is the negative of the log-likelihood (minimize loss = maximize likelihood) and has the following form:\n",
    "\n",
    "\\begin{align}\n",
    "\\large\\textbf{L}(\\theta) &= \\large-\\frac{1}{n}\\sum_{i=1}^n y_i \\log\\left(\\sigma(\\phi(x_i)^T \\theta) \\right) +\n",
    "(1-y_i) \\log\\left(1 - \\sigma(\\phi(x_i)^T\\theta)\\right)\\\\\n",
    "& \\large = -\\frac{1}{n}\\sum_{i=1}^n  y_i \\phi(x_i)^T \\theta +\n",
    "\\log\\left(\\sigma(-\\phi(x_i)^T\\theta) \\right)\n",
    "\\end{align}\n",
    "\n",
    "We can implement this in the following python expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_loss(theta, Phi, Y):\n",
    "    t = Phi @ theta\n",
    "    return -np.mean(Y * t - np.log(1 + np.exp(t)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By evaluating the loss at a grid of points we can can construct the loss surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uvalues = np.linspace(1,8,70)\n",
    "vvalues = np.linspace(-5,5,70)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "thetas = np.vstack((u.flatten(),v.flatten()))\n",
    "lr_loss_values = np.array([lr_loss(t, Phi, Y) for t in thetas.T])\n",
    "lr_loss_surface = go.Surface(name=\"Logistic Regression Loss\",\n",
    "        x=u, y=v, z=np.reshape(lr_loss_values,(len(uvalues), len(vvalues))),\n",
    "        contours=dict(z=dict(show=True, color=\"gray\", project=dict(z=True)))\n",
    "    )\n",
    "optimal_batch_lr_point = go.Scatter3d(name = \"Optimal Point\",\n",
    "        x = [theta_batch[0]], y = [theta_batch[1]], \n",
    "        z = [lr_loss(theta_batch, Phi, Y)],\n",
    "        marker=dict(size=10, color=\"red\")\n",
    "    )\n",
    "py.iplot(go.Figure(data=[lr_loss_surface, optimal_batch_lr_point]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the gradient descent path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we plot the solution path taken by gradient descent.  \n",
    "\n",
    "However, to make the plot easy to visualize (solve in fewer more consistent iterations) I introduced gradient clipping. Here I constrain the gradient norm to ensure that it is never larger than one.  This prevents the solution from exploding in the first few iterations. For those who are interested in optimization techniques this is a form of **proximal** gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gd_thetas(X, Y, theta0, grad, max_iter = 1000000,  \n",
    "                    epsilon=0.01, clipping=True, rho0 = 1.):\n",
    "    theta = theta0\n",
    "    thetas = [theta0]\n",
    "    n = len(X)\n",
    "    for t in range(1, max_iter):\n",
    "        rho = rho0 / t\n",
    "        g = grad(theta, X, Y)\n",
    "        # To make the solution (and plot) more stable \n",
    "        # I have constrained the gradient step size to\n",
    "        # the unit ball.  This is sometimes called \n",
    "        # gradient clipping.\n",
    "        if clipping and np.linalg.norm(g) > 1:\n",
    "            g = g / np.linalg.norm(g)\n",
    "        theta = theta - rho * g\n",
    "        thetas.append(theta)\n",
    "        # Track Convergence\n",
    "        if np.linalg.norm(g) < epsilon:\n",
    "            return np.array(thetas)\n",
    "    return np.array(thetas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path = batch_gd_thetas(Phi, Y, np.array([3, 2.5]), gradL)\n",
    "\n",
    "thata_points = go.Scatter(name=\"Theta Values\", x=theta_path[:,0], y=theta_path[:,1],\n",
    "                          mode=\"lines+markers\")\n",
    "lr_loss_contours = go.Contour(name=\"Logistic Regression Loss\",\n",
    "        x=uvalues, y=vvalues, z=np.reshape(np.log(lr_loss_values), (len(uvalues), len(vvalues))),\n",
    "        colorscale='Viridis', reversescale=True\n",
    "    )\n",
    "\n",
    "optimal_batch_lr_point = go.Scatter(name = \"Optimal Point\",\n",
    "        x = [theta_batch[0]], y = [theta_batch[1]],\n",
    "        marker=dict(size=10, color=\"red\")\n",
    "    )\n",
    "\n",
    "py.iplot(go.Figure(data=[lr_loss_contours, thata_points, optimal_batch_lr_point]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a Bigger Initial Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path = batch_gd_thetas(Phi, Y, np.array([3, 2.5]), gradL, rho0=5.0)\n",
    "\n",
    "thata_points = go.Scatter(name=\"Theta Values\", x=theta_path[:,0], y=theta_path[:,1],\n",
    "                          mode=\"lines+markers\")\n",
    "lr_loss_contours = go.Contour(name=\"Logistic Regression Loss\",\n",
    "        x=uvalues, y=vvalues, z=np.reshape(np.log(lr_loss_values), (len(uvalues), len(vvalues))),\n",
    "        colorscale='Viridis', reversescale=True\n",
    "    )\n",
    "\n",
    "optimal_batch_lr_point = go.Scatter(name = \"Optimal Point\",\n",
    "        x = [theta_batch[0]], y = [theta_batch[1]],\n",
    "        marker=dict(size=10, color=\"red\")\n",
    "    )\n",
    "\n",
    "py.iplot(go.Figure(data=[lr_loss_contours, thata_points, optimal_batch_lr_point], \n",
    "                   layout=dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "In the following we implement **stochastic gradient descent (SGD)** to solve the logistic regression problem.  Afterwards we use much more efficient and robust libraries (and you should do the same).  However we implement SGD here for pedagogical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(X,Y, theta0, grad, max_iter = 5000, batch_size=50, epsilon=0.0001):\n",
    "    theta = theta0\n",
    "    n = len(X)\n",
    "    for t in range(1, max_iter):\n",
    "        rho = 1./t\n",
    "        i = t % n\n",
    "        g = n/batch_size * grad(theta, X[i:(i+batch_size),], Y[i:(i+batch_size)])\n",
    "        if np.linalg.norm(g) > 1:\n",
    "            g = g / np.linalg.norm(g)\n",
    "        theta = theta - rho * g\n",
    "        # Track Convergence\n",
    "        if (t % 10000) == 0:\n",
    "            if np.linalg.norm(grad(theta, X, Y)) < epsilon:\n",
    "                return theta\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the theta values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_sgd = sgd(Phi, Y, np.array([2.5, 2.5]), gradL)\n",
    "theta_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the SGD Solution Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the solution path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_thetas(X,Y, theta0, grad, max_iter = 5000, batch_size=50, epsilon=0.0001,\n",
    "              clipping=True, rho0 = 1.):\n",
    "    theta = theta0\n",
    "    n = len(X)\n",
    "    thetas = [theta]\n",
    "    for t in range(1, max_iter):\n",
    "        rho = rho0/t\n",
    "        i = t % n\n",
    "        g = n/batch_size * grad(theta, X[i:(i+batch_size),], Y[i:(i+batch_size)])\n",
    "        if clipping and np.linalg.norm(g) > 1:\n",
    "            g = g / np.linalg.norm(g)\n",
    "        theta = theta - rho * g\n",
    "        thetas.append(theta)\n",
    "    return np.array(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_path = sgd_thetas(Phi, Y, np.array([2.5, 2.5]), gradL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thata_points = go.Scatter(name=\"Theta Values\", x=theta_path[:,0], y=theta_path[:,1],\n",
    "        mode=\"lines+markers\", showlegend=False\n",
    "    )\n",
    "lr_loss_contours = go.Contour(name=\"Logistic Regression Loss\",\n",
    "        x=uvalues, y=vvalues, z=np.reshape(lr_loss_values,(len(uvalues), len(vvalues))),\n",
    "        colorscale='Viridis', reversescale=True, \n",
    "    )\n",
    "optimal_batch_lr_point = go.Scatter(name = \"Optimal Point\",\n",
    "        x = [theta_sgd[0]], y = [theta_sgd[1]],\n",
    "        marker=dict(size=10, color=\"red\"), showlegend=False\n",
    "    )\n",
    "py.iplot(go.Figure(data=[lr_loss_contours, thata_points, optimal_batch_lr_point]), filename=\"lr-13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a larger initial step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_path = sgd_thetas(Phi, Y, np.array([2.5, 2.5]), gradL, rho0=5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thata_points = go.Scatter(name=\"Theta Values\", x=theta_path[:,0], y=theta_path[:,1],\n",
    "        mode=\"lines+markers\", showlegend=False\n",
    "    )\n",
    "lr_loss_contours = go.Contour(name=\"Logistic Regression Loss\",\n",
    "        x=uvalues, y=vvalues, z=np.reshape(lr_loss_values,(len(uvalues), len(vvalues))),\n",
    "        colorscale='Viridis', reversescale=True, \n",
    "    )\n",
    "optimal_batch_lr_point = go.Scatter(name = \"Optimal Point\",\n",
    "        x = [theta_sgd[0]], y = [theta_sgd[1]],\n",
    "        marker=dict(size=10, color=\"red\"), showlegend=False\n",
    "    )\n",
    "py.iplot(go.Figure(data=[lr_loss_contours, thata_points, optimal_batch_lr_point]), filename=\"lr-13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_path = sgd_thetas(Phi, Y, np.array([2.5, 2.5]), gradL, rho0=5., batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thata_points = go.Scatter(name=\"Theta Values\", x=theta_path[:,0], y=theta_path[:,1],\n",
    "        mode=\"lines+markers\", showlegend=False\n",
    "    )\n",
    "lr_loss_contours = go.Contour(name=\"Logistic Regression Loss\",\n",
    "        x=uvalues, y=vvalues, z=np.reshape(lr_loss_values,(len(uvalues), len(vvalues))),\n",
    "        colorscale='Viridis', reversescale=True, \n",
    "    )\n",
    "optimal_batch_lr_point = go.Scatter(name = \"Optimal Point\",\n",
    "        x = [theta_sgd[0]], y = [theta_sgd[1]],\n",
    "        marker=dict(size=10, color=\"red\"), showlegend=False\n",
    "    )\n",
    "py.iplot(go.Figure(data=[lr_loss_contours, thata_points, optimal_batch_lr_point]), filename=\"lr-13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = ff.create_distplot([\n",
    "    data[~data['malignant']]['mean radius'],\n",
    "    data[data['malignant']]['mean radius']\n",
    "], group_labels=['benign','malignant'],  bin_size=0.5)\n",
    "plt.data.append(go.Scatter(name=\"Logistic Reg. Decision Boundary\", \n",
    "                      x = [lr_dec_boundary,lr_dec_boundary], y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"red\", dash=\"dot\")))\n",
    "plt.data.append(go.Scatter(name=\"Least Squares Decision Boundary\", \n",
    "                      x = [dec_boundary,dec_boundary], y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"black\", dash=\"dot\")))\n",
    "\n",
    "lr_sgd_dec_boundary = (0.0 - theta_sgd[1])/theta_sgd[0]\n",
    "lr_sgd_dec_boundary = standardizer.inverse_transform([lr_sgd_dec_boundary])[0]\n",
    "plt.data.append(go.Scatter(name=\"SGD Logistic Reg. Dec. Boundary\", \n",
    "                      x = [lr_sgd_dec_boundary,lr_sgd_dec_boundary], \n",
    "                           y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"blue\", dash=\"dot\")))\n",
    "py.iplot(plt, filename=\"lr-14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Separability\n",
    "\n",
    "The built in Scikit learn logistic regression models use $L^2$ regularization by default (a good idea).\n",
    "\n",
    "1. Why might regularization be necessary?\n",
    "1. What happens if the data are **separable**: there is a plane that separates one class from another?\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtoy = np.hstack([-np.random.rand(20) - 0.5, np.random.rand(20) + 0.5])\n",
    "Ytoy = np.hstack([np.zeros(20), np.ones(20)])\n",
    "Phi_toy = np.hstack([np.array([Xtoy]).T, np.ones([len(Xtoy),1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_theta = gradient_descent(Phi_toy, Ytoy, \n",
    "                             np.array([1,-1]), gradL,\n",
    "                             epsilon=0.0001)\n",
    "toy_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if the data are separable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy_plt = np.hstack([\n",
    "    np.array([np.linspace(-2,2,100)]).T, \n",
    "    np.ones((100,1))])\n",
    "py.iplot([\n",
    "    go.Scatter(name = \"Zeros\", x=Xtoy[0:20], y=Ytoy[0:20] + 0.01 * np.random.randn(20), \n",
    "               mode=\"markers\", marker=dict(opacity=0.5, color=\"blue\")),\n",
    "    go.Scatter(name =\"Ones\", x=Xtoy[20:], y=Ytoy[20:] + 0.01 * np.random.randn(20), \n",
    "               mode=\"markers\", marker=dict(opacity=0.5, color=\"red\")),\n",
    "    go.Scatter(name=\"Logistic Regression\",\n",
    "    x=np.linspace(-2,2,100), y=sigmoid(X_toy_plt @ toy_theta), \n",
    "    mode=\"lines\", line=dict(color=\"green\", width=4))\n",
    "], filename=\"lr-15\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we make theta bigger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigger_toy_theta = toy_theta*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy_plt = np.hstack([\n",
    "    np.array([np.linspace(-2,2,200)]).T, \n",
    "    np.ones((200,1))])\n",
    "py.iplot([\n",
    "    go.Scatter(name = \"Zeros\", x=Xtoy[0:20], y=Ytoy[0:20] + 0.01 * np.random.randn(20), \n",
    "               mode=\"markers\", marker=dict(opacity=0.5, color=\"blue\")),\n",
    "    go.Scatter(name =\"Ones\", x=Xtoy[20:], y=Ytoy[20:] + 0.01 * np.random.randn(20), \n",
    "               mode=\"markers\", marker=dict(opacity=0.5, color=\"red\")),\n",
    "    go.Scatter(name=\"Logistic Regression\",\n",
    "                x=np.linspace(-2,2,200), y=sigmoid(X_toy_plt @ toy_theta), \n",
    "                mode=\"lines\", line=dict(color=\"green\", width=4)),\n",
    "    go.Scatter(name=\"Logistic Regression Bigger Theta\",\n",
    "        x=np.linspace(-2,2,200), y=sigmoid(X_toy_plt @ bigger_toy_theta), \n",
    "    mode=\"lines\", line=dict(color=\"red\", width=4))\n",
    "], filename=\"lr-15\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_loss(bigger_toy_theta, Phi_toy, Ytoy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the built in cross-validation support for logistic regression in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LogisticRegressionCV(100)\n",
    "lr.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_line = go.Scatter(name=\"Scikit-LR\",\n",
    "    x=X_plt, y=lr.predict_proba(np.array([X_plt]).T).T[1], \n",
    "    mode=\"lines\", line=dict(color=\"green\", width=4))\n",
    "py.iplot([points, lr_line], filename=\"lr-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = ff.create_distplot([\n",
    "    data[~data['malignant']]['mean radius'],\n",
    "    data[data['malignant']]['mean radius']\n",
    "], group_labels=['benign', 'malignant'],  bin_size=0.5)\n",
    "plt.data.append(go.Scatter(name=\"Logistic Reg. Decision Boundary\", \n",
    "                      x = [lr_dec_boundary,lr_dec_boundary], y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"red\", dash=\"dot\")))\n",
    "plt.data.append(go.Scatter(name=\"Least Squares Decision Boundary\", \n",
    "                      x = [dec_boundary,dec_boundary], y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"black\", dash=\"dot\")))\n",
    "plt.data.append(go.Scatter(name=\"SGD Logistic Reg. Dec. Boundary\", \n",
    "                      x = [lr_sgd_dec_boundary,lr_sgd_dec_boundary], \n",
    "                           y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"blue\", dash=\"dot\")))\n",
    "lr_sk_dec_boundary = (0.0 - lr.intercept_[0])/lr.coef_[0,0]\n",
    "\n",
    "plt.data.append(go.Scatter(name=\"SK Logistic Reg. Dec. Boundary\", \n",
    "                      x = [lr_sk_dec_boundary,lr_sk_dec_boundary], \n",
    "                           y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"pink\", dash=\"dot\")))\n",
    "\n",
    "py.iplot(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Prediction accuracy\n",
    "\n",
    "Comparing the prediction accuracy with least squares we see a bit of an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "lr_errors = cross_val_score(linear_model.LogisticRegression(), X, Y, \n",
    "                scoring=make_scorer(zero_one_loss), cv=5)\n",
    "\n",
    "print(\"LR Error:\", np.mean(lr_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "print(\"Least Squares Error:\", \n",
    "      zero_one_loss(Y, least_squares_model.predict(X) > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Always predict 0:\", zero_one_loss(Y, np.zeros(Y.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Prediction & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We temporarily return to a simple synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "nk = 50\n",
    "Xmc = np.vstack([\n",
    "    np.random.randn(nk,2) + np.array([3,1]),\n",
    "    np.random.randn(nk,2) + np.array([1,3]),\n",
    "    0.5 * np.random.randn(nk,2) ,\n",
    "    0.5 * np.random.randn(nk,2) + np.array([3,3])\n",
    "])\n",
    "Ymc = np.hstack([np.zeros(nk), np.ones(nk), 2*np.ones(nk),2*np.ones(nk)])\n",
    "\n",
    "blue_points = go.Scatter(name = \"A\", x=Xmc[Ymc==0,0], y=Xmc[Ymc==0,1], \n",
    "               mode=\"markers\", marker=dict(opacity=0.5, color=\"blue\"))\n",
    "red_points  = go.Scatter(name = \"B\", x=Xmc[Ymc==1,0], y=Xmc[Ymc==1,1], \n",
    "               mode=\"markers\", marker=dict(opacity=0.5, color=\"red\"))\n",
    "orange_points = go.Scatter(name = \"C\", x=Xmc[Ymc==2,0], y=Xmc[Ymc==2,1], \n",
    "               mode=\"markers\", marker=dict(opacity=0.5, color=\"orange\"))\n",
    "\n",
    "\n",
    "\n",
    "py.iplot([blue_points, red_points, orange_points], filename=\"lr-18\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Max Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a soft-max classifier.  Currently scikit learn requires that we use an LBFGS solver for soft-max.  L-BFGS is a quasi second order method that uses the both the gradient and a diagonal approximation of the second derivative (the Hessian) to solve for the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "lr.fit(Xmc,Ymc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Decision Surface\n",
    "\n",
    "In the following we plot the corresponding decision surface generated by the basic multiclass logistic regression model.  Notice that the limitations of the linear decision surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uvalues = np.linspace(-4,7,70)\n",
    "vvalues = np.linspace(-3,7,70)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "coords = np.vstack((u.flatten(),v.flatten())).T\n",
    "label = lr.predict(coords)\n",
    "label_contour = go.Contour(\n",
    "    name=\"Label ContourSurace\",\n",
    "    x = uvalues, y=vvalues, z=np.reshape(label,(len(uvalues), len(vvalues))),\n",
    "    colorscale=[[0.0, 'rgb(100,100,100)'], [0.5, 'rgb(150,150,150)'], [1.0, 'rgb(200,200,200)']]\n",
    ")\n",
    "py.iplot(go.Figure(data=[label_contour, blue_points, red_points, orange_points]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering with Gaussian RBFs\n",
    "\n",
    "To capture the non-linear structure we randomly sample 20 data points and create an RBF feature centered at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_rbf(us, lam=1):\n",
    "    return lambda x: np.array([np.exp(-np.linalg.norm(x - u)**2 / lam**2) for u in us])\n",
    "\n",
    "num_basis = 20\n",
    "np.random.seed(42)\n",
    "rbf_features = gaussian_rbf(Xmc[np.random.choice(range(len(Xmc)), num_basis, replace=False),:])\n",
    "Phimc = np.array([rbf_features(x) for x in Xmc])\n",
    "\n",
    "lr_rbf = linear_model.LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\")\n",
    "lr_rbf.fit(Phimc,Ymc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again fitting the logistic regression model we get a non-linear decision surface in our original space but a linear decision surface in a higher dimensional transformed space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uvalues = np.linspace(-4,7,70)\n",
    "vvalues = np.linspace(-3,7,70)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "coords = np.array([rbf_features(x) for x in np.vstack((u.flatten(),v.flatten())).T])\n",
    "label = lr_rbf.predict(coords)\n",
    "label_contour = go.Contour(\n",
    "    name=\"Label ContourSurace\",\n",
    "    x = uvalues, y=vvalues, z=np.reshape(label,(len(uvalues), len(vvalues))),\n",
    "    colorscale=[[0.0, 'rgb(100,100,100)'], [0.5, 'rgb(150,150,150)'], [1.0, 'rgb(200,200,200)']]\n",
    ")\n",
    "py.iplot(go.Figure(data=[label_contour, blue_points, red_points, orange_points]), \n",
    "         filename=\"lr-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uvalues = np.linspace(-4,7,70)\n",
    "vvalues = np.linspace(-3,7,70)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "coords = np.array([rbf_features(x) for x in np.vstack((u.flatten(),v.flatten())).T])\n",
    "prb = lr_rbf.predict_proba(coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import tools\n",
    "fig = tools.make_subplots(rows=1, cols=3,\n",
    "                          specs=[[{'is_3d': True}, {'is_3d': True}, {'is_3d': True}]])\n",
    "\n",
    "for i in range(3):\n",
    "    surf = go.Surface(name=\"Class \" + str(i+1),\n",
    "                      x=u, y=v, z=np.reshape(prb[:,i],(len(uvalues), len(vvalues))), \n",
    "                      showscale = False)\n",
    "    fig.append_trace(surf, 1, i+1)\n",
    "# fig['layout'].update(height=600, width=600, title='i <3 subplots')\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
