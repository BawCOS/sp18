{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:50.660687Z",
     "start_time": "2018-03-29T21:17:48.840143Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=False)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=False, world_readable=True, theme='ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear Relationships\n",
    "\n",
    "For this exercise we are going to use synthetic data to illustrate the basic ideas of model design. Notice here that we are generating data from a linear model with Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:50.695715Z",
     "start_time": "2018-03-29T21:17:50.663707Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/toy_training_data.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:50.708484Z",
     "start_time": "2018-03-29T21:17:50.697815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the data ---------------------\n",
    "train_points = go.Scatter(name = \"Training Data\", \n",
    "                          x = train_data['X'], y = train_data['Y'], \n",
    "                          mode = 'markers')\n",
    "# layout = go.Layout(autosize=False, width=800, height=600)\n",
    "py.iplot(go.Figure(data=[train_points]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Normal Equations\n",
    "\n",
    "There are several ways to compute the normal equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the Data as Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:50.719850Z",
     "start_time": "2018-03-29T21:17:50.710638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note that we select a list of columns to return a matrix (n,p)\n",
    "X = train_data[['X']].values\n",
    "print(\"X shape:\", X.shape)\n",
    "\n",
    "Y = train_data['Y'].values\n",
    "print(\"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the ones column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:50.726271Z",
     "start_time": "2018-03-29T21:17:50.721731Z"
    }
   },
   "outputs": [],
   "source": [
    "Phi = np.hstack([X, np.ones((len(X), 1))])\n",
    "Phi[1:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving for $\\hat{\\theta{}}$\n",
    "\n",
    "There are multiple ways to solve for $\\hat{\\theta{}}$. Following the solution to the normal equations:\n",
    "\n",
    "$$\\large\n",
    " \\hat{\\theta{}} = \\left(\\Phi^T  \\Phi\\right)^{-1} \\Phi^T Y\n",
    "$$\n",
    "\n",
    "we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:50.999621Z",
     "start_time": "2018-03-29T21:17:50.728690Z"
    }
   },
   "outputs": [],
   "source": [
    "theta_hat = np.linalg.inv(Phi.T @ Phi) @ Phi.T @ Y\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However computing inverting and multiplying (i.e., solving) can be accomplished with a special routine more efficiently:\n",
    "\n",
    "$$\\large\n",
    "A^{-1}b = \\texttt{solve}(A, b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.006710Z",
     "start_time": "2018-03-29T21:17:51.001790Z"
    }
   },
   "outputs": [],
   "source": [
    "theta_hat = np.linalg.solve(Phi.T @ Phi, Phi.T @ Y)\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Package to estimate the linear model\n",
    "\n",
    "In practice, it is generally better to use specialized software packages for linear regression.  In Python, [scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares) is the standard package for regression.  \n",
    "\n",
    "![scikit-learn logo](http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png)\n",
    "\n",
    "Here we will take a very brief tour of how to use scikit-learn for regression.  Over the next few weeks we will use scikit-learn for a range of different task.\n",
    "\n",
    "You can use the the [scikit-learn `linear_model` package](http://scikit-learn.org/stable/modules/linear_model.html) to compute the normal equations. This package supports a wide range of **generalized linear models**.  For those who are interested in studying machine learning, I would encourage you to skim through the descriptions of the various models in the `linear_model` package. These are the foundation of most practical applications of supervised machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.169807Z",
     "start_time": "2018-03-29T21:17:51.008641Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intercept Term**  Scikit-learn can automatically add the intercept term.  This can be helpful since you don't need to remember to add it later when making a prediction.  In the following we create a model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.174684Z",
     "start_time": "2018-03-29T21:17:51.171892Z"
    }
   },
   "outputs": [],
   "source": [
    "line_reg = linear_model.LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then fit the model in one line (this solves the normal equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.205538Z",
     "start_time": "2018-03-29T21:17:51.177989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model to the data\n",
    "line_reg.fit(train_data[['X']], train_data['Y'])\n",
    "np.hstack([line_reg.coef_, line_reg.intercept_]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the solution\n",
    "\n",
    "In the following we plot the solution along with it's residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions Using Linear Algebra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions at each of the training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.212556Z",
     "start_time": "2018-03-29T21:17:51.207270Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = Phi @ theta_hat\n",
    "y_hat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the trained model to render predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.221485Z",
     "start_time": "2018-03-29T21:17:51.214862Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = line_reg.predict(train_data[['X']])\n",
    "y_hat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the fit \n",
    "\n",
    "To visualize the fit line we will make a set of predictions at 10 evenly spaced points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.229775Z",
     "start_time": "2018-03-29T21:17:51.223727Z"
    }
   },
   "outputs": [],
   "source": [
    "X_query = np.linspace(train_data['X'].min()-1, train_data['X'].max() +1, 1000)\n",
    "Phi_query = np.hstack([X_query[:,np.newaxis], np.ones((len(X_query),1))])\n",
    "y_query_pred = Phi_query @ theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the residuals along with the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.305509Z",
     "start_time": "2018-03-29T21:17:51.232086Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the least squares regression line \n",
    "basic_line = go.Scatter(name = r\"Linear Model\", x=X_query, y = y_query_pred)\n",
    "\n",
    "# Definethe residual lines segments, a separate line for each \n",
    "# training point\n",
    "residual_lines = [\n",
    "    go.Scatter(x=[x,x], y=[y,yhat],\n",
    "               mode='lines', showlegend=False, \n",
    "               line=dict(color='black', width = 0.5))\n",
    "    for (x, y, yhat) in zip(train_data['X'], train_data['Y'], y_hat)\n",
    "]\n",
    "\n",
    "# Combine the plot elements\n",
    "py.iplot([train_points, basic_line] + residual_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Residuals\n",
    "\n",
    "It is often helpful to examine the residuals.  Ideally the residuals should be normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.523033Z",
     "start_time": "2018-03-29T21:17:51.307408Z"
    }
   },
   "outputs": [],
   "source": [
    "residuals = train_data['Y'] - y_hat\n",
    "sns.distplot(residuals)\n",
    "# plt.savefig(\"residuals.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also plot $\\hat{Y}$ vs $Y$.  Ideally, the data should be along the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.536852Z",
     "start_time": "2018-03-29T21:17:51.525519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot.ly plotting code\n",
    "py.iplot(go.Figure(\n",
    "    data = [go.Scatter(x=train_data['Y'], y=y_hat, mode='markers', name=\"Points\"),\n",
    "            go.Scatter(x=[-10, 50], y = [-10, 50], line=dict(color='black'), \n",
    "                       name=\"Diagonal\", mode='lines')],\n",
    "    layout = dict(title=r\"Y_hat vs Y Plot\", xaxis=dict(title=\"Y\"), \n",
    "                  yaxis=dict(title=r\"$\\hat{Y}$\"))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of the residuals should be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.542955Z",
     "start_time": "2018-03-29T21:17:51.538604Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting against dimensions in X\n",
    "\n",
    "When plotted against any one dimension we don't want to see any patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.554351Z",
     "start_time": "2018-03-29T21:17:51.545682Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot.ly plotting code\n",
    "py.iplot(go.Figure(\n",
    "    data = [go.Scatter(x=train_data['X'], y=residuals, mode='markers')],\n",
    "    layout = dict(title=\"Residual Plot\", xaxis=dict(title=\"X\"), \n",
    "                  yaxis=dict(title=\"Residual\"))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Is this data linear?\n",
    "\n",
    "** How would you describe this data? **\n",
    "1. Is there a linear relationship between $X$ and $Y$?\n",
    "1. Are there other patterns?\n",
    "1. How noisy is the data?\n",
    "1. Do we see obvious outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The relationship between X and Y does appear to have some linear trend but there also appears to be other patterns in the relationship? \n",
    "\n",
    "However, in this lecture we will show that linear models can still be used to model this data very effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "---\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "## Question: *Can we fit this non-linear cyclic structure with a linear model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Yes!  Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "# What does it mean to be a _linear model_\n",
    "\n",
    "\n",
    "Let's return to what it means to be a linear model:\n",
    "\n",
    "$$\\large\n",
    "f_\\theta(x) = x^T \\theta = \\sum_{j=1}^p x_j \\theta_j\n",
    "$$\n",
    "\n",
    "In what sense is the above model **linear**? \n",
    "1. Linear in the features $x$? \n",
    "1. Linear in the parameters $\\theta$?\n",
    "1. Linear in both at the same time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Yes, Yes, and No.  If we look at just the features or just the parameters the model is linear. However, if we look at both at the same time it is not. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "# Feature Functions\n",
    "\n",
    "Consider the following alternative model formulation:\n",
    "\n",
    "$$\\large\n",
    "f_\\theta\\left( x \\right) = \\phi(x)^T \\theta = \\sum_{j=1}^{k} \\phi_j(x) \\theta_j\n",
    "$$\n",
    "\n",
    "where $\\phi_j$ is an *arbitrary function* from $x\\in \\mathbb{R}^p$ to $\\phi(x)_j \\in \\mathbb{R}$ and we define $k$ of these functions.  We often refer to these functions $\\phi_j$ as **feature functions** or **basis functions** and their design plays a critical role in both how we capture prior knowledge and our ability to fit complicated data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "# The Transformed Covariate Matrix\n",
    "\n",
    "As a consequence, while the model $f_\\theta\\left(x \\right)$ is no longer linear in $x$ it is still a **linear model** because it _is linear in $\\theta$_.  This means we can continue to use the normal equations to compute the optimal parameters.  \n",
    "\n",
    "Minimizing the squared loss (not shown) we obtain the **normal equation**:\n",
    "\n",
    "$$ \\large\n",
    "\\hat{\\theta} = \\left( \\Phi^T \\Phi \\right)^{-1} \\Phi^T Y\n",
    "$$\n",
    "\n",
    "It is worth noting that the model is also linear in $\\phi$ and that the $\\phi_j$ form a new **basis** (hence the term **basis functions**) in which the data live.  As a consequence we can think of $\\phi$ as mapping the data into a new (often higher dimensional space) in which the relationship between $y$ and $\\phi(x)$ is defined by a **hyperplane**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "# Capturing Domain Knowledge\n",
    "\n",
    "\n",
    "Feature functions can be used to capture domain knowledge by:\n",
    "1. Introducing additional information from other sources\n",
    "1. Combining related features\n",
    "1. Encoding non-linear patterns\n",
    "\n",
    "Suppose I had data about customer purchases and I wanted to estimate their income:\n",
    "\n",
    "\\begin{align}\n",
    "\\phi(\\text{date}, \\text{lat}, \\text{lon}, \\text{amount})_1 \n",
    "&= \\textbf{isWinter}(\\text{date}) \\\\\n",
    "\\phi(\\text{date}, \\text{lat}, \\text{lon}, \\text{amount})_2 \n",
    "&= \\cos\\left( \\frac{\\textbf{Hour}(\\text{date})}{12} \\pi \\right)  \\\\\n",
    "\\phi(\\text{date}, \\text{lat}, \\text{lon}, \\text{amount})_3 \n",
    "&= \\frac{\\text{amount}}{\\textbf{avg_spend}[\\textbf{ZipCode}[\\text{lat}, \\text{lon}]]} \\\\\n",
    "\\phi(\\text{date}, \\text{lat}, \\text{lon}, \\text{amount})_4 \n",
    "&= \\exp\\left(-\\textbf{Distance}\\left((\\text{lat},\\text{lon}),  \\textbf{StoreA}\\right)\\right)^2 \\\\\n",
    "\\phi(\\text{date}, \\text{lat}, \\text{lon}, \\text{amount})_5 \n",
    "&= \\exp\\left(-\\textbf{Distance}\\left((\\text{lat},\\text{lon}),  \\textbf{StoreB}\\right)\\right)^2 \n",
    "\\end{align}\n",
    "\n",
    "**Notice:** In the above feature functions:\n",
    "1. The transformations are non-linear\n",
    "1. They pull in additional information\n",
    "1. They may encode implicit knowledge\n",
    "1. The functions $\\phi$ **do not** depend on $\\theta$\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "# Transforming the Toy Data\n",
    "\n",
    "In our toy data set we observed a cyclic pattern.  Here we construct a $\\phi$ to capture the cyclic nature of our data and visualize the corresponding hyperplane.\n",
    "\n",
    "\n",
    "In the following cell we define a function $\\phi$ that maps $x\\in \\mathbb{R}$ to the vector $[x,\\sin(x)] \\in \\mathbb{R}^2$ \n",
    "\n",
    "$$ \\large\n",
    "\\phi(x) = [x, \\sin(x)]\n",
    "$$\n",
    "\n",
    "**Why not:**\n",
    "\n",
    "$$ \\large\n",
    "\\phi(x) = [x, \\sin(\\theta_3 x + \\theta_4)]\n",
    "$$\n",
    "\n",
    "This would no longer be linear $\\theta$.  However, in practice we might want to consider a range of $\\sin$ basis:\n",
    "\n",
    "$$ \\large\n",
    "\\phi_{\\alpha,\\beta}(x) = \\sin(\\alpha x + \\beta)\n",
    "$$\n",
    "\n",
    "for different values of $\\alpha$ and $\\beta$.  The parameters $\\alpha$ and $\\beta$ are typically called **hyperparameters** because (at least in this setting) they are not set automatically through learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.559368Z",
     "start_time": "2018-03-29T21:17:51.556356Z"
    }
   },
   "outputs": [],
   "source": [
    "def sin_phi(x):\n",
    "    return np.hstack([x, np.sin(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.568540Z",
     "start_time": "2018-03-29T21:17:51.561659Z"
    }
   },
   "outputs": [],
   "source": [
    "Phi = sin_phi(train_data[[\"X\"]])\n",
    "Phi[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a _Linear Model_ on $\\Phi$\n",
    "\n",
    "We can again use the scikit-learn package to fit a linear model on the transformed space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Basic Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.577727Z",
     "start_time": "2018-03-29T21:17:51.570438Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "basic_reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "basic_reg.fit(train_data[['X']], train_data['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Transformed Linear Model on $\\Phi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.591139Z",
     "start_time": "2018-03-29T21:17:51.579950Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "sin_reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "sin_reg.fit(sin_phi(train_data[[\"X\"]]), train_data['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions at Query Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.597866Z",
     "start_time": "2018-03-29T21:17:51.593065Z"
    }
   },
   "outputs": [],
   "source": [
    "X_query = np.linspace(train_data['X'].min()-1, train_data['X'].max() +1, 100)\n",
    "Y_basic_query = basic_reg.predict(X_query[:, np.newaxis])\n",
    "Y_sin_query = sin_reg.predict(sin_phi(X_query[:, np.newaxis]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.660232Z",
     "start_time": "2018-03-29T21:17:51.600048Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the least squares regression line \n",
    "basic_line = go.Scatter(name = r\"Basic Model\", x=X_query, y = Y_basic_query)\n",
    "sin_line = go.Scatter(name = r\"Transformed Model\", x=X_query, y = Y_sin_query)\n",
    "\n",
    "# Definethe residual lines segments, a separate line for each \n",
    "# training point\n",
    "residual_lines = [\n",
    "    go.Scatter(x=[x,x], y=[y,yhat],\n",
    "               mode='lines', showlegend=False, \n",
    "               line=dict(color='black', width = 0.5))\n",
    "    for (x, y, yhat) in zip(train_data['X'], train_data['Y'], \n",
    "                            sin_reg.predict(sin_phi(train_data[[\"X\"]])))\n",
    "]\n",
    "\n",
    "# Combine the plot elements\n",
    "py.iplot([train_points, basic_line, sin_line] + residual_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model in a Transformed Space\n",
    "\n",
    "As discussed earlier the model we just constructed, while non-linear in $x$ is actually a linear model in $\\phi(x)$ and we can visualize that linear model's structure in higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.759220Z",
     "start_time": "2018-03-29T21:17:51.662174Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the data in higher dimensions\n",
    "phi3d = go.Scatter3d(name = \"Raw Data\",\n",
    "    x = Phi[:,0], y = Phi[:,1], z = train_data['Y'],\n",
    "    mode = 'markers',\n",
    "    marker = dict(size=3),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Compute the predictin plane\n",
    "(u,v) = np.meshgrid(np.linspace(-10,10,5), np.linspace(-1,1,5))\n",
    "coords = np.vstack((u.flatten(),v.flatten())).T\n",
    "ycoords = sin_reg.predict(coords)\n",
    "fit_plane = go.Surface(name = \"Fitting Hyperplane\",\n",
    "    x = np.reshape(coords[:,0], (5,5)),\n",
    "    y = np.reshape(coords[:,1], (5,5)),\n",
    "    z = np.reshape(ycoords, (5,5)),\n",
    "    opacity = 0.8, cauto = False, showscale = False,\n",
    "    colorscale = [[0, 'rgb(255,0,0)'], [1, 'rgb(255,0,0)']]\n",
    ")\n",
    "\n",
    "# Construct residual lines\n",
    "Yhat = sin_reg.predict(Phi)\n",
    "residual_lines = [\n",
    "    go.Scatter3d(x=[x[0],x[0]], y=[x[1],x[1]], z=[y, yhat],\n",
    "                 mode='lines', showlegend=False, \n",
    "                 line=dict(color='black'))\n",
    "    for (x, y, yhat) in zip(Phi, train_data['Y'], Yhat)\n",
    "]\n",
    "\n",
    "    \n",
    "# Label the axis and orient the camera\n",
    "layout = go.Layout(\n",
    "    scene=go.Scene(\n",
    "        xaxis=go.XAxis(title='X'),\n",
    "        yaxis=go.YAxis(title='sin(X)'),\n",
    "        zaxis=go.ZAxis(title='Y'),\n",
    "        aspectratio=dict(x=1.,y=1., z=1.), \n",
    "        camera=dict(eye=dict(x=-1, y=-1, z=0))\n",
    "    )\n",
    ")\n",
    "\n",
    "py.iplot(go.Figure(data=[phi3d, fit_plane] + residual_lines, layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Estimates\n",
    "\n",
    "How well are we fitting the data?  We can compute the root mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.764076Z",
     "start_time": "2018-03-29T21:17:51.761101Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmse(y, yhat):\n",
    "    return np.sqrt(np.mean((yhat-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.773187Z",
     "start_time": "2018-03-29T21:17:51.766167Z"
    }
   },
   "outputs": [],
   "source": [
    "basic_rmse = rmse(train_data['Y'], basic_reg.predict(train_data[['X']]))\n",
    "sin_rmse = rmse(train_data['Y'], sin_reg.predict(sin_phi(train_data[['X']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.784776Z",
     "start_time": "2018-03-29T21:17:51.775817Z"
    }
   },
   "outputs": [],
   "source": [
    "py.iplot(go.Figure(data =[go.Bar(\n",
    "            x=[r'Basic Regression', \n",
    "               r'Sine Transformation'],\n",
    "            y=[basic_rmse, sin_rmse]\n",
    "            )], layout = go.Layout(title=\"Loss Comparison\", \n",
    "                           yaxis=dict(title=\"RMSE\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Feature Functions\n",
    "\n",
    "We will now explore a range of generic feature transformations. However, before we proceed it is worth contrasting two categories of feature functions and their applications.\n",
    "\n",
    "1. **Interpretable Features:** In settings where our goal is to understand the model (e.g., identify important features that predict customer churn) we may want to construct meaningful features based on our understanding of the domain.  \n",
    "\n",
    "1. **Generic Features:** However, in other settings where our primary goals is to make accurate predictions we may instead introduce generic feature functions that enable our models to fit _and generalize_ complex relationships. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Radial Basis Functions\n",
    "\n",
    "One of the more widely used generic feature functions are Gaussian radial basis functions.  These feature functions take the form:\n",
    "\n",
    "$$\n",
    "\\phi_{(\\lambda, u_1, \\ldots, u_k)}(x) = \\left[\\exp\\left( - \\frac{\\left|\\left|x-u_1\\right|\\right|_2^2}{\\lambda} \\right), \\ldots, \n",
    "\\exp\\left( - \\frac{\\left|\\left| x-u_k \\right|\\right|_2^2}{\\lambda} \\right) \\right]\n",
    "$$\n",
    "\n",
    "The **hyper-parameters** $u_1$ through $u_k$ and $\\lambda$ are not optimized with $\\theta$ but instead are set externally.  In many cases the $u_i$ may correspond to points in the training data.  The term $\\lambda$ defines the spread of the basis function and determines the \"smoothness\" of the function $f_\\theta(\\phi(x))$.\n",
    "\n",
    "The following is a plot of three radial basis function centered at 2 with different values of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.790706Z",
     "start_time": "2018-03-29T21:17:51.787206Z"
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_rbf(u, lam=1):\n",
    "    return lambda x: np.exp(-(x - u)**2 / lam**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.855513Z",
     "start_time": "2018-03-29T21:17:51.793240Z"
    }
   },
   "outputs": [],
   "source": [
    "tmpX = np.linspace(-2, 6,1000)\n",
    "py.iplot([\n",
    "    dict(name=r\"$\\lambda=0.5$\", x=tmpX, \n",
    "         y=gaussian_rbf(2, lam=0.5)(tmpX)),\n",
    "    dict(name=r\"$\\lambda=1$\", x=tmpX, \n",
    "         y=gaussian_rbf(2, lam=1.)(tmpX)),\n",
    "    dict(name=r\"$\\lambda=2$\", x=tmpX, \n",
    "         y=gaussian_rbf(2, lam=2.)(tmpX))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop some helper code\n",
    "\n",
    "To simplify the following analysis we create two helper functions. \n",
    "\n",
    "1. `uniform_rbf_phi` which constructs uniformly spaced RBF functions and each function is a feature that has a large value when the input $x$ is nearby. \n",
    "1. `evaluate_basis` which takes a feature function configuration and fits a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.860123Z",
     "start_time": "2018-03-29T21:17:51.857199Z"
    }
   },
   "outputs": [],
   "source": [
    "def uniform_rbf_phi(x, lam=1, num_basis = 10, minvalue=-9, maxvalue=9):\n",
    "    return np.hstack([gaussian_rbf(u, lam)(x) for u in np.linspace(minvalue, maxvalue, num_basis)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.919549Z",
     "start_time": "2018-03-29T21:17:51.861806Z"
    }
   },
   "outputs": [],
   "source": [
    "tmpXTall = np.linspace(-11, 11,1000)[:,np.newaxis]\n",
    "py.iplot([\n",
    "    dict(name=r\"$\\lambda=0.1$\", x=tmpXTall[:,0], \n",
    "         y=uniform_rbf_phi(tmpXTall, lam=0.1).mean(axis=1)),\n",
    "    dict(name=r\"$\\lambda=0.5$\", x=tmpXTall[:,0], \n",
    "         y=uniform_rbf_phi(tmpXTall, lam=0.5).mean(axis=1)),\n",
    "    dict(name=r\"$\\lambda=1$\", x=tmpXTall[:,0], \n",
    "         y=uniform_rbf_phi(tmpXTall, lam=1.).mean(axis=1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.938320Z",
     "start_time": "2018-03-29T21:17:51.921150Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_basis(phi, desc):\n",
    "    # Apply transformation\n",
    "    Phi = phi(train_data[[\"X\"]])\n",
    "    \n",
    "    # Fit a model\n",
    "    reg_model = linear_model.LinearRegression(fit_intercept=True)\n",
    "    reg_model.fit(Phi, train_data['Y'])\n",
    "    \n",
    "    # Create plot line\n",
    "    X_test = np.linspace(-11, 11, 1000) # Fine grained test X\n",
    "    Phi_test = phi(X_test[:,np.newaxis])\n",
    "    Yhat_test = reg_model.predict(Phi_test)\n",
    "    line = go.Scatter(name = desc, x=X_test, y=Yhat_test)\n",
    "    \n",
    "    # Compute RMSE\n",
    "    Yhat = reg_model.predict(Phi)\n",
    "    error = rmse(train_data['Y'], Yhat)\n",
    "    \n",
    "    # return results\n",
    "    return (line, error, reg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing 10 RBF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:51.986330Z",
     "start_time": "2018-03-29T21:17:51.940150Z"
    }
   },
   "outputs": [],
   "source": [
    "(rbf_line10, rbf_rmse10, rbf_reg10) = \\\n",
    "    evaluate_basis(lambda x: uniform_rbf_phi(x, lam=1, num_basis=10), r\"RBF10\")\n",
    "\n",
    "py.iplot([train_points, rbf_line10, basic_line, sin_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing 50 RBF features (Really *Connecting the Dots*!)\n",
    "\n",
    "We are now getting a really good fit to this dataset!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:52.083400Z",
     "start_time": "2018-03-29T21:17:51.988031Z"
    }
   },
   "outputs": [],
   "source": [
    "(rbf_line50, rbf_rmse50, rbf_reg50) = \\\n",
    "    evaluate_basis(lambda x: uniform_rbf_phi(x, lam=0.3, num_basis=50), r\"RBF50\")\n",
    "\n",
    "fig = go.Figure(data=[train_points, rbf_line50, rbf_line10, sin_line, basic_line ], \n",
    "                   layout = go.Layout(xaxis=dict(range=[-10,10]), \n",
    "                                      yaxis=dict(range=[-10,50])))\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:52.094000Z",
     "start_time": "2018-03-29T21:17:52.085406Z"
    }
   },
   "outputs": [],
   "source": [
    "train_bars = go.Bar(\n",
    "            x=[r'Basic Regression', \n",
    "               r'Sine Transformation',\n",
    "               r'RBF 10',\n",
    "               r'RBF 50'],\n",
    "            y=[basic_rmse, sin_rmse, rbf_rmse10, rbf_rmse50],\n",
    "            name=\"Training Erorr\")\n",
    "py.iplot(go.Figure(data = [train_bars], layout = go.Layout(title=\"Loss Comparison\", \n",
    "                           yaxis=dict(title=\"RMSE\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which is the best model?\n",
    "\n",
    "We started with the objective of minimizing the training loss (error).  As we increased the model sophistication by adding features we were able to fit increasingly complex functions to the data and reduce the loss.  However, is our ultimate goal to minimize training error?  \n",
    "\n",
    "Ideally we would like to minimize the error we make when making new predictions at unseen values of $X$.  One way to evaluate that error is use a **test dataset** which is distinct from the dataset used to train the model.  Fortunately, we have such a test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:52.108621Z",
     "start_time": "2018-03-29T21:17:52.096051Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/toy_test_data.csv\")\n",
    "\n",
    "test_points = go.Scatter(name = \"Test Data\", x = test_data['X'], y = test_data['Y'], \n",
    "                       mode = 'markers', marker=dict(symbol=\"cross\", color=\"red\"))\n",
    "py.iplot([train_points, test_points])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:17:52.113619Z",
     "start_time": "2018-03-29T21:17:52.110634Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_rmse(phi, reg):\n",
    "    return rmse(test_data['Y'], reg.predict(phi(test_data[['X']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T21:18:06.373983Z",
     "start_time": "2018-03-29T21:18:06.312066Z"
    }
   },
   "outputs": [],
   "source": [
    "test_bars = go.Bar(\n",
    "            x=[r'Basic Regression', \n",
    "               r'Sine Transformation',\n",
    "               r'RBF 10',\n",
    "               r'RBF 50'],\n",
    "            y=[test_rmse(lambda x: x, basic_reg),\n",
    "               test_rmse(sin_phi, sin_reg),\n",
    "               test_rmse(lambda x: uniform_rbf_phi(x, lam=1, num_basis=10), rbf_reg10),\n",
    "               test_rmse(lambda x: uniform_rbf_phi(x, lam=0.3, num_basis=50), rbf_reg50)\n",
    "              ],\n",
    "            name=\"Test Error\"\n",
    "            )\n",
    "py.iplot(go.Figure(data =[train_bars, test_bars], layout = go.Layout(title=\"Loss Comparison\", \n",
    "                           yaxis=dict(title=\"RMSE\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's happening: _Over-fitting_\n",
    "\n",
    "As we increase the expressiveness of our model we begin to **over-fit** to the variability in our training data.  That is we are learning patterns that do not **generalize** beyond our training dataset\n",
    "\n",
    "**Over-fitting** is a key challenge in machine learning and statistical inference.  At it's core is a fundamental trade-off between **bias** and **variance**: _the desire to explain the training data and yet be robust to variation in the training data_.\n",
    "\n",
    "We will study the **bias-variance** trade-off more in the next lecture but for now we will focus on the trade-off between under fitting and over fitting:\n",
    "\n",
    "<img src=\"images/under_over_fitting.png\" width=\"500px\">\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Test, and Validation Split\n",
    "\n",
    "To manage over-fitting it is essential to split your initial training data into a training and testing dataset.  \n",
    "\n",
    "<img src=\"images/train_test_split.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Train - Test Split\n",
    "\n",
    "Before running cross validation split the data into train and test subsets (typically a 90-10 split). How should you do this?  You want the test data to reflect the prediction goal:\n",
    "1. **Random** sample of the training data\n",
    "1. **Future** examples\n",
    "1. Different **stratifications**\n",
    "\n",
    "Ask yourself, where will I be using this model and how does that relate to my test data.\n",
    "\n",
    "**Do not look at the test data until after selecting your final model.** Also, it is very important to **not look at the test data until after selecting your final model.**  Finally, you should **not look at the test data until after selecting your final model.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "_With the **remaining training data**:_\n",
    "1. Split the remaining **training dataset** k-ways  as in the Figure above.  The figure uses 5-Fold which is standard.  You should split the data in the same way you constructed the test set (this is typically randomly)\n",
    "1. For each split train the model on the training fraction and then compute the error (RMSE) on the validation fraction.\n",
    "1. Average the error across each validation fraction to _estimate_ the test error.\n",
    "\n",
    "**Questions:**\n",
    "1. What is this accomplishing?\n",
    "1. What are the implication on the choice of $k$? \n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "\n",
    "\n",
    "**Answers:**\n",
    "1. This is repeatedly simulating the train-test split we did earlier.  We repeat this process because it is noisy.\n",
    "1. **Larger $k$** means we average our validation error over more instances which makes our estimate of the test error **more stable**.  This typically also means that the validation set is smaller so we have more training data.  However, larger $k$ also means we have to train the model more often which gets computational expensive\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
