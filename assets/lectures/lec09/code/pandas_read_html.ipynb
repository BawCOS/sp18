{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Downloading Tables\n",
    "\n",
    "In this notebook we review some of the basic tools you might use to pull data from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:10.457534Z",
     "start_time": "2018-02-13T03:29:09.782113Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Solar Energy Usage:\n",
    "\n",
    "Suppose I am interested in looking at photovoltaic output for California.  This data is aggregated on the web here:\n",
    "\n",
    "http://www.energy.ca.gov/almanac/renewables_data/solar/index.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Web Tables with Pandas\n",
    "\n",
    "The Pandas library has basic support for loading tables directly from websites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:14.875732Z",
     "start_time": "2018-02-13T03:29:11.548325Z"
    }
   },
   "outputs": [],
   "source": [
    "tables = pd.read_html(\"http://www.energy.ca.gov/almanac/renewables_data/solar/index.php\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a list of tables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:14.884219Z",
     "start_time": "2018-02-13T03:29:14.878200Z"
    }
   },
   "outputs": [],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a little data from each table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:14.941342Z",
     "start_time": "2018-02-13T03:29:14.886536Z"
    }
   },
   "outputs": [],
   "source": [
    "for t in tables:\n",
    "    display(t.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we return to the website here: \n",
    "\n",
    "http://www.energy.ca.gov/almanac/renewables_data/solar/index.php \n",
    "\n",
    "we see that this reflects the content on the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Backup!\n",
    "Save a backup of the data in case we need it again later ... (The web changes!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:14.948562Z",
     "start_time": "2018-02-13T03:29:14.943413Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"energy_download.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tables, f,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to save large data files would be to use something like the hd5 format.  To do this you need to install  `PyTables`:\n",
    "```bash\n",
    "!conda install -y pytables\n",
    "```\n",
    "then run the following:\n",
    "```python\n",
    "for i, t in enumerate(tables):\n",
    "    t.to_hdf(\"energy.h5\", \"table_\" + str(i))\n",
    "```\n",
    "Note that this has the limitation that the table needs to be cleaned slightly to be stored efficiently (e.g., mapping types)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the Photovoltaic Table\n",
    "\n",
    "Looking at the tables above it appears that the first table contains `Solar Thermal` data and the second contains `Solar PV` data.  We could rely on this ordering assumption but the order of tables may change.  Let's instead write. function to extract the table by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:14.957080Z",
     "start_time": "2018-02-13T03:29:14.951014Z"
    }
   },
   "outputs": [],
   "source": [
    "tables[0].iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:14.965383Z",
     "start_time": "2018-02-13T03:29:14.959401Z"
    }
   },
   "outputs": [],
   "source": [
    "[t.iloc[0,0] for t in tables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a little helper function to find the table by **name** for at least the first two tables.  We could rely on location but if things move it might break our code **silently**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:14.972155Z",
     "start_time": "2018-02-13T03:29:14.967581Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_table(name, tables):\n",
    "    return {t.iloc[0,0]: t for t in tables}[name].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:14.987336Z",
     "start_time": "2018-02-13T03:29:14.974301Z"
    }
   },
   "outputs": [],
   "source": [
    "pv_table = find_table(\"Solar PV\", tables)\n",
    "pv_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:15.004076Z",
     "start_time": "2018-02-13T03:29:14.989587Z"
    }
   },
   "outputs": [],
   "source": [
    "thermal_table = find_table(\"Solar Thermal\", tables)\n",
    "thermal_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "When extracting data from the web with Pandas we are likely to need to do substantial cleaning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:15.432888Z",
     "start_time": "2018-02-13T03:29:15.427724Z"
    }
   },
   "outputs": [],
   "source": [
    "pv_table.iloc[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:16.164646Z",
     "start_time": "2018-02-13T03:29:16.159975Z"
    }
   },
   "outputs": [],
   "source": [
    "pv_table.columns = pv_table.iloc[1,:]\n",
    "pv_table.columns.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I reset name of the columns.  Why?  Notice that the series object has a name as well (row `1`).  I don't want `1` to be the name of the column index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:17.513454Z",
     "start_time": "2018-02-13T03:29:17.501136Z"
    }
   },
   "outputs": [],
   "source": [
    "pv_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Header Rows from the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:19.635443Z",
     "start_time": "2018-02-13T03:29:19.632329Z"
    }
   },
   "outputs": [],
   "source": [
    "pv_table = pv_table.iloc[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:19.844080Z",
     "start_time": "2018-02-13T03:29:19.829832Z"
    }
   },
   "outputs": [],
   "source": [
    "pv_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Bottom of the Table for Summaries\n",
    "\n",
    "It is common on human readable tables to see summary statistics at the bottom.  We don't want to treat these as rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:20.350403Z",
     "start_time": "2018-02-13T03:29:20.336748Z"
    }
   },
   "outputs": [],
   "source": [
    "pv_table.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:21.803149Z",
     "start_time": "2018-02-13T03:29:21.799726Z"
    }
   },
   "outputs": [],
   "source": [
    "pv_table = pv_table[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:22.020237Z",
     "start_time": "2018-02-13T03:29:22.005082Z"
    }
   },
   "outputs": [],
   "source": [
    "pv_table.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Data Types\n",
    "\n",
    "Pandas may not have correctly inferred the types of numeric fields.  However run statistics on these fields and create visualizations we will need their type information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:22.407553Z",
     "start_time": "2018-02-13T03:29:22.401567Z"
    }
   },
   "outputs": [],
   "source": [
    "pv_table.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:23.681534Z",
     "start_time": "2018-02-13T03:29:23.676118Z"
    }
   },
   "outputs": [],
   "source": [
    "pv_table = pv_table.astype({\"Year\": \"int\", \"Net MWh\": \"float\", \"Capacity (MW)\": \"float\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:23.866623Z",
     "start_time": "2018-02-13T03:29:23.860766Z"
    }
   },
   "outputs": [],
   "source": [
    "pv_table.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Generic Cleaning Function\n",
    "\n",
    "Because we need to clean two roughly identical tables with the same structure we will create a simple function that we can debug once and run on both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:24.178318Z",
     "start_time": "2018-02-13T03:29:24.172130Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_solar_table(table):\n",
    "    table = table.copy()\n",
    "    # Extract and set the column names\n",
    "    table.columns = table.iloc[1,:].values\n",
    "    # drop headers and summary at end\n",
    "    table = table.iloc[2:-1]\n",
    "    # Change types\n",
    "    table = table.astype({\"Year\": \"int\", \"Net MWh\": \"float\", \"Capacity (MW)\": \"float\"})\n",
    "    return table.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:25.520403Z",
     "start_time": "2018-02-13T03:29:25.498606Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_solar_table(thermal_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the Thermal and PV Data\n",
    "\n",
    "Both tables contain the same data.  Let's take the union of all the rows adding an extra column to indicate whether the record is a `Thermal` or `PV` record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:26.087822Z",
     "start_time": "2018-02-13T03:29:26.082354Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_and_combine_pv_and_thermal(tables):\n",
    "    thermal_table = clean_solar_table(find_table(\"Solar Thermal\", tables))\n",
    "    pv_table = clean_solar_table(find_table(\"Solar PV\", tables))\n",
    "    thermal_table[\"Kind\"] = \"Thermal\"\n",
    "    pv_table[\"Kind\"] = \"PV\"\n",
    "    return pd.concat([thermal_table, pv_table]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:26.854669Z",
     "start_time": "2018-02-13T03:29:26.822359Z"
    }
   },
   "outputs": [],
   "source": [
    "df = extract_and_combine_pv_and_thermal(tables)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:29.507896Z",
     "start_time": "2018-02-13T03:29:29.497194Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(\"Kind\")[[\"Capacity (MW)\", \"Net MWh\"]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a backup\n",
    "Let's save the data at this point in case we need to recover it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:29:46.164293Z",
     "start_time": "2018-02-13T03:29:46.155807Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"combined_energy_part1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
