{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:41.793696Z",
     "start_time": "2018-02-01T05:19:40.202099Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# ## Plotly plotting support\n",
    "# import plotly.plotly as py\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=False)\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "\n",
    "cf.set_config_file(offline=False, world_readable=True, theme='ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lecture we examine the process of data cleaning and Exploratory Data Analysis (EDA).  Often you will acquire or even be given a collection of data in order to conduct some analysis or answer some questions. The first step in using that data is to ensure that it is in the correct form (cleaned) and that you understand its properties and limitations (EDA).  Often as you explore data through EDA you will identify additional transformations that may be required before the data is ready for analysis.\n",
    "\n",
    "In this notebook we obtain crime data from the city of Berkeley's public records.  Ultimately, our goal might be to understand policing patterns but before we get there we must first clean and understand the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Data\n",
    "\n",
    "To begin this analysis we want to get data about crimes in Berkeley.  Remarkably, the city of Berkeley maintains an [Open Data Portal](https://data.cityofberkeley.info/) for citizens to access data about the city.  We will be examining the:\n",
    "\n",
    "1. [Call Data](https://data.cityofberkeley.info/Public-Safety/Berkeley-PD-Calls-for-Service/k2nh-s5h5)\n",
    "1. [Stop Data](https://data.cityofberkeley.info/Public-Safety/Berkeley-PD-Stop-Data/6e9j-pj9p)\n",
    "\n",
    "Fortunately, this data is also relatively well document with detailed descriptions of what it contains.  Here are summaries of the fields in the data:\n",
    "\n",
    "### Calls Data\n",
    "<img src=\"calls_desc.png\" width=800px />\n",
    "\n",
    "### Stop Data\n",
    "<img src=\"stops_desc.png\" width=800px />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Most data has bad documentation:\n",
    "\n",
    "Unfortunately, data is seldom well documented and when it is you may not be able to trust the documentation. It is therefore critical that when we download the data we investigate the fields and verify that it reflects the assumptions made in the documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible Data Science\n",
    "\n",
    "In the interest of **reproducible data science** we will download the data programatically.  We have defined some helper functions in the [utils.py](utils.py) file.  I can then reuse these helper functions in many different notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:41.798898Z",
     "start_time": "2018-02-01T05:19:41.795408Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import fetch_and_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:41.803884Z",
     "start_time": "2018-02-01T05:19:41.800856Z"
    }
   },
   "outputs": [],
   "source": [
    "help(fetch_and_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can actually look at the source code in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:41.836074Z",
     "start_time": "2018-02-01T05:19:41.805385Z"
    }
   },
   "outputs": [],
   "source": [
    "fetch_and_cache??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally, you will want to modify code that you have imported.  To reimport those modifications you can either use the python importlib library:\n",
    "\n",
    "```python\n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "```\n",
    "\n",
    "or use iPython magic which will intelligently import code when files change:\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data\n",
    "\n",
    "Notice that because I record how I got the data in the notebook, others can reproduce this experiment.  However, it is worth noting that **the data can change**.  We will want to pay attention to file timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:41.841871Z",
     "start_time": "2018-02-01T05:19:41.837963Z"
    }
   },
   "outputs": [],
   "source": [
    "calls_file = fetch_and_cache(\"https://data.cityofberkeley.info/api/views/k2nh-s5h5/rows.csv?accessType=DOWNLOAD\",\n",
    "                \"calls_for_service.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:41.847218Z",
     "start_time": "2018-02-01T05:19:41.843713Z"
    }
   },
   "outputs": [],
   "source": [
    "stops_file = fetch_and_cache(\"https://data.cityofberkeley.info/resource/4p7k-drdw.json?$limit=100000\",\n",
    "                \"stops.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrated live downloading (of something we won't need in lecture) we can download professor Gonzalez's website.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:41.992194Z",
     "start_time": "2018-02-01T05:19:41.849637Z"
    }
   },
   "outputs": [],
   "source": [
    "gonzalez_file = fetch_and_cache(\"https://people.eecs.berkeley.edu/~jegonzal/assets/jegonzal.jpg\", \n",
    "                                \"gonzalez.jpg\", \n",
    "                                force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data\n",
    "\n",
    "Now that we have obtained the data we want to understand its:\n",
    "\n",
    "* **Structure** -- the \"shape\" of a data file\n",
    "* **Granularity** -- how fine/coarse is each datum\n",
    "* **Scope** -- how (in)complete is the data\n",
    "* **Temporality** -- how is the data situated in time\n",
    "* **Faithfulness** -- how well does the data capture \"reality\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "Before we even begin to load the data it often helps to understand a little about the high-level structure:\n",
    "\n",
    "1. How much data do I have?\n",
    "1. How is it formatted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big is the data?\n",
    "\n",
    "I often like to start my analysis by getting a rough estimate of the size of the data.  This will help inform the tools I use and how I view the data.  If it is relatively small I might use a text editor or a spreadsheet to look at the data.  If it is larger, I might jump to more programmatic exploration or even used distributed computing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:41.998953Z",
     "start_time": "2018-02-01T05:19:41.993971Z"
    }
   },
   "outputs": [],
   "source": [
    "type(stops_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.009623Z",
     "start_time": "2018-02-01T05:19:42.000948Z"
    }
   },
   "outputs": [],
   "source": [
    "help(stops_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.014644Z",
     "start_time": "2018-02-01T05:19:42.011229Z"
    }
   },
   "outputs": [],
   "source": [
    "stops_file.stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.028544Z",
     "start_time": "2018-02-01T05:19:42.016987Z"
    }
   },
   "outputs": [],
   "source": [
    "calls_file.stat().st_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the files are relatively small and we could comfortable examine them in a text editors.  (Personally, I like *sublime* or *emacs* but others may have a different *vi*ew.). \n",
    "\n",
    "In listing the files I noticed that the names suggest that they are all text file formats:\n",
    "* **CSV**: Comma separated values is a very standard table format.\n",
    "* **JSON**: JavaScript Object Notation is a very standard semi-structured file format used to store nested data.\n",
    "\n",
    "We will dive into the formats in a moment.  However because these are text data I might also want to investigate the number of lines which often correspond to records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.072360Z",
     "start_time": "2018-02-01T05:19:42.030297Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import line_count\n",
    "help(line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.087656Z",
     "start_time": "2018-02-01T05:19:42.073943Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Lines in the calls file\", line_count(calls_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.107253Z",
     "start_time": "2018-02-01T05:19:42.089617Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Lines in the stops file\", line_count(stops_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your iPython session is running on a [POSIX](https://en.wikipedia.org/wiki/POSIX) compliant machine (e.g., mac/linux) you could also run:\n",
    "\n",
    "```bash\n",
    "!wc data/*\n",
    "```\n",
    "\n",
    "which would produce (lines, words, characters):\n",
    "\n",
    "```\n",
    "   16053   85512  976004 data/calls_for_service.csv\n",
    "     148    1541   54988 data/gonzalez.jpg\n",
    "   29850  658525 6123225 data/stops.json\n",
    "   46051  745578 7154217 total\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.111647Z",
     "start_time": "2018-02-01T05:19:42.109223Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wc data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to learn more about the command we could use the `man` (short for **manual**) command to read the manual page for `wc` (word count)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional File Meta-Data\n",
    "\n",
    "Meta-data is data about the data.  Most filesystems will maintain information about when a file was created, read, or modified.  Below we use the Python `time` library to print the time in the local representation.  Note, time is often measured in [Unix Time](https://en.wikipedia.org/wiki/Unix_time) (time in second since January 1st, 1970).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.117290Z",
     "start_time": "2018-02-01T05:19:42.113248Z"
    }
   },
   "outputs": [],
   "source": [
    "import time \n",
    "print(\"Time of creation (seconds since Epoch):\", \n",
    "      stops_file.stat().st_ctime)\n",
    "print(\"Local time of creation:\", \n",
    "      time.ctime(stops_file.stat().st_ctime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is the file format?  (Can we trust extensions?)\n",
    "\n",
    "We already noticed that the files end in `csv` and `json` which suggests that these are comma separated and javascript object files respectively.  However, we can't always rely on the naming as this is only a convention.  For example, here we picked the name of the file when downloading based on some hints in the URL.\n",
    "\n",
    "\n",
    "\n",
    "**Often files will have incorrect extensions or no extension at all.**\n",
    "\n",
    "Let's assume that these are text files (and do not contain binary encoded data) so we can print a \"few lines\" to get a better understanding of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.122383Z",
     "start_time": "2018-02-01T05:19:42.119023Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import head\n",
    "help(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.127412Z",
     "start_time": "2018-02-01T05:19:42.123952Z"
    }
   },
   "outputs": [],
   "source": [
    "head(calls_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Because the `gonzalez_file` is a JPEG (image) it is not a character file format and looking at the head of the file isn't going to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.133230Z",
     "start_time": "2018-02-01T05:19:42.128936Z"
    }
   },
   "outputs": [],
   "source": [
    "head(stops_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.136799Z",
     "start_time": "2018-02-01T05:19:42.134905Z"
    }
   },
   "outputs": [],
   "source": [
    "# head(gonzalez_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some observations about `Calls` data?\n",
    "\n",
    "1. It appears to be in comma separated value (CSV) format.\n",
    "1. First line contains the column headings.\n",
    "1. There are lots of **new-line** `\\n` characters:\n",
    "    * at the ends of lines (delimiting records?)\n",
    "    * *within records* as part of addresses.\n",
    "1. There are **\"quoted\"** strings in the `Block_Location` column:\n",
    "```\n",
    "\"2500 LE CONTE AVE\n",
    "Berkeley, CA\n",
    "(37.876965, -122.260544)\"\n",
    "```\n",
    "these are going to be difficult.  What are the implications on our earlier line count calculations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.142394Z",
     "start_time": "2018-02-01T05:19:42.138394Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "head(calls_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some observations about `Stops` data?\n",
    "\n",
    "This appears to be a fairly standard JSON file.  We notice that the file appears to contain a description of itself in a field called \"meta\" (which is presumably short for meta-data).  We will come back to this meta data in a moment but first let's quickly discuss the JSON file format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.148212Z",
     "start_time": "2018-02-01T05:19:42.144196Z"
    }
   },
   "outputs": [],
   "source": [
    "head(stops_file, lines=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A quick note on JSON\n",
    "\n",
    "[JSON (JavaScript Object Notation)](http://www.json.org) is a common format for exchanging complex structured and semi-structured data. \n",
    "\n",
    "```javascript\n",
    "{\n",
    "    \"field1\": \"value1\",\n",
    "    \"field2\": [\"list\", \"of\", \"values\"],\n",
    "    \"myfield3\": {\"is_recursive\": true, \"a null value\": null}\n",
    "}\n",
    "```\n",
    "\n",
    "A few key points:\n",
    "* JSON is a recursive format in that JSON fields can also contain JSON objects\n",
    "* JSON closely matches Python Dictionaries:\n",
    "```python\n",
    "d = {\n",
    "    \"field1\": \"value1\",\n",
    "    \"field2\": [\"list\", \"of\", \"values\"],\n",
    "    \"myfield3\": {\"is_recursive\": True, \"a null value\": None}\n",
    "}\n",
    "print(d['myfield3'])\n",
    "```\n",
    "* Very common in web technologies (... JavaScript)\n",
    "* Many languages have tools for loading and saving JSON objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading the Data\n",
    "\n",
    "We will now attempt to load the data into python.  We will be using the Pandas dataframe library for basic tabular data analysis.  Fortunately, the Pandas library has some relatively sophisticated functions for loading data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loading the Calls Data\n",
    "\n",
    "Because the file appears to be a relatively well formatted CSV we will attempt to load it directly and allow the Pandas Library to deduce column headers.  (Always check that first row and column look correct after loading.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.209779Z",
     "start_time": "2018-02-01T05:19:42.149667Z"
    }
   },
   "outputs": [],
   "source": [
    "calls = pd.read_csv(calls_file)\n",
    "calls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records did we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.217223Z",
     "start_time": "2018-02-01T05:19:42.211890Z"
    }
   },
   "outputs": [],
   "source": [
    "calls.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Preliminary observations on the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "1. `EVENTDT` -- Contain the incorrect time stamp\n",
    "1. `EVENTTM` -- Contains the time in 24 hour format (What timezone?)\n",
    "1. `CVDOW` -- Appears to be some encoding of the day of the week (see data documentation).\n",
    "1. `InDbDate` -- Appears to be correctly formatted and appears pretty consistent in time.\n",
    "1. **`Block_Location` -- Errr, what a mess!  newline characters, and Geocoordinates all merged!!  Fortunately, this field was \"quoted\" otherwise we would have had trouble parsing the file. (why?)**\n",
    "1. `BLKADDR` -- This appears to be the address in Block Location.\n",
    "1. `City` and `State` seem redundant given this is supposed to be the city of Berkeley dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Checking that the City and State fields are redundant\n",
    "\n",
    "We notice that there are city and state columns.  Since this is supposed to be data for the city of Berkeley these columns appear to be redundant.  Let's quickly compute the number of occurences of unique values for these two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.240165Z",
     "start_time": "2018-02-01T05:19:42.219087Z"
    }
   },
   "outputs": [],
   "source": [
    "calls.groupby([\"City\", \"State\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding day of the week\n",
    "\n",
    "According to the documentation `CVDOW=0` is Sunday, `CVDOW=1` is Monday, ...,  Therefore we can make a series to decode the day of the week for each record and join that series with the calls data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.247083Z",
     "start_time": "2018-02-01T05:19:42.242100Z"
    }
   },
   "outputs": [],
   "source": [
    "dow = pd.Series([\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"], name=\"DoW\")\n",
    "dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.268225Z",
     "start_time": "2018-02-01T05:19:42.248696Z"
    }
   },
   "outputs": [],
   "source": [
    "calls['Day'] = dow[calls['CVDOW']].reset_index(drop=True)\n",
    "calls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also accomplish the same task using a join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.296635Z",
     "start_time": "2018-02-01T05:19:42.270121Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dow = pd.DataFrame(dow)\n",
    "pd.merge(calls, df_dow, left_on='CVDOW', right_index=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Block Location\n",
    "\n",
    "The block location contains the GPS coordinates and I would like to use these to analyze the location of each request.  Let's try to extract the GPS coordinates using regular expressions (we will cover regular expressions in future lectures):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.302831Z",
     "start_time": "2018-02-01T05:19:42.298547Z"
    }
   },
   "outputs": [],
   "source": [
    "calls['Block_Location'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.332714Z",
     "start_time": "2018-02-01T05:19:42.305333Z"
    }
   },
   "outputs": [],
   "source": [
    "calls_lat_lon = (\n",
    "    # Remove newlines\n",
    "    calls['Block_Location'].str.replace(\"\\n\", \"\\t\") \n",
    "    # Extract Lat and Lon using regular expression\n",
    "    .str.extract(\".*\\((?P<Lat>\\d*\\.\\d*)\\, (?P<Lon>-?\\d*.\\d*)\\)\",expand=True)\n",
    ")\n",
    "calls_lat_lon.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check that all of the records have a latitude and longitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.342686Z",
     "start_time": "2018-02-01T05:19:42.335111Z"
    }
   },
   "outputs": [],
   "source": [
    "calls_lat_lon.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code joins the extracted Latitude and Longitude fields with the calls data.  Notice that we actually drop these fields before joining.  This is to enable repeated invocation of this cell even after the join has been completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.374026Z",
     "start_time": "2018-02-01T05:19:42.344591Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove Lat and Lon if they already existed before (reproducible)\n",
    "calls.drop([\"Lat\", \"Lon\"], axis=1, inplace=True, errors=\"ignore\")\n",
    "# Join in the the latitude and longitude data\n",
    "calls = calls.merge(calls_lat_lon, left_index=True, right_index=True)\n",
    "# calls[[\"Lat\", \"Lon\"]] = calls_lat_lon\n",
    "# calls.join(calls_lat_lon)\n",
    "calls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at a few of the records that were missing latitude and longitude entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.406717Z",
     "start_time": "2018-02-01T05:19:42.377910Z"
    }
   },
   "outputs": [],
   "source": [
    "calls[calls['Lat'].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "## Loading the `stops.json` Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has relatively good support for JSON data since it closely matches the internal python object model.  In the following cell we import the entire JSON datafile into a python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.507298Z",
     "start_time": "2018-02-01T05:19:42.409110Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/stops.json\", \"rb\") as f:\n",
    "    stops_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stops_json` variable is now a dictionary encoding the data in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.513679Z",
     "start_time": "2018-02-01T05:19:42.509700Z"
    }
   },
   "outputs": [],
   "source": [
    "type(stops_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "### We can now examine what keys are in the top level json object\n",
    "\n",
    "We can list the keys to determine what data is stored in the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.519662Z",
     "start_time": "2018-02-01T05:19:42.515597Z"
    }
   },
   "outputs": [],
   "source": [
    "stops_json.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "The JSON dictionary contains a `meta` key which likely refers to meta data (data about the data).  Meta data often maintained with the data and can be a good source of additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "## Digging into Meta Data\n",
    "\n",
    "We can investigate the meta data further by examining the keys associated with the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.526353Z",
     "start_time": "2018-02-01T05:19:42.521475Z"
    }
   },
   "outputs": [],
   "source": [
    "stops_json['meta'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `meta` key contains another dictionary called `view`.  This likely refers to meta-data about a particular \"view\" of some underlying database.  We will learn more about views as we study SQL later in the class.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.532430Z",
     "start_time": "2018-02-01T05:19:42.528314Z"
    }
   },
   "outputs": [],
   "source": [
    "stops_json['meta']['view'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this a nested/recursive data structure.  As we dig deeper we reveal more and more keys and the corresponding data:\n",
    "\n",
    "```\n",
    "meta\n",
    "|-> data\n",
    "    | ... (haven't explored yet)\n",
    "|-> view\n",
    "    | -> id\n",
    "    | -> name\n",
    "    | -> attribution \n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a key called description in the view sub dictionary.  This likely contains a description of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.537621Z",
     "start_time": "2018-02-01T05:19:42.534558Z"
    }
   },
   "outputs": [],
   "source": [
    "print(stops_json['meta']['view']['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "### Columns Meta data\n",
    "\n",
    "Another potentially useful key in the meta data dictionary is the `columns`.  This returns a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.543735Z",
     "start_time": "2018-02-01T05:19:42.539601Z"
    }
   },
   "outputs": [],
   "source": [
    "type(stops_json['meta']['view']['columns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can brows the list using python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.552563Z",
     "start_time": "2018-02-01T05:19:42.545750Z"
    }
   },
   "outputs": [],
   "source": [
    "for c in stops_json['meta']['view']['columns']:\n",
    "    print(c[\"name\"], \"----------->\")\n",
    "    if \"description\" in c:\n",
    "        print(c[\"description\"])\n",
    "    print(\"======================================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Observations?\n",
    "\n",
    "1. The above meta data tells us a lot about the columns in the data including both column names and even descriptions.  This information will be useful in loading and working with the data.\n",
    "1. JSON makes it easier (than CSV) to create \"self-documented data\". \n",
    "1. Self documenting data can be helpful since it maintains it's own description and these descriptions are more likely to be updated as data changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Examining the Data Field\n",
    "\n",
    "We can look at a few entires in the data field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.558546Z",
     "start_time": "2018-02-01T05:19:42.554405Z"
    }
   },
   "outputs": [],
   "source": [
    "stops_json['data'][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Dataframe from JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block of code we:\n",
    "1. Translate the JSON records into a dataframe\n",
    "1. Remove columns that have no metadata description.  This would be a bad idea in general but here we remove these columns since the above analysis suggests that they are unlikely to contain useful information.\n",
    "1. Examine the top of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.631896Z",
     "start_time": "2018-02-01T05:19:42.560759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data from JSON and assign column titles\n",
    "stops = pd.DataFrame(\n",
    "    stops_json['data'],\n",
    "    columns=[c['name'] for c in stops_json['meta']['view']['columns']])\n",
    "\n",
    "stops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Observations\n",
    "\n",
    "What do we observe so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:\n",
    "1. The `Incident Number` appears to have the year encoded in it.  \n",
    "1. The `Call Date/Time` Field looks to be formatted in YYYY-MM-DDTHH:MM:SS.  I am guessing T means \"Time\".\n",
    "1. `Incident Type` has some weird coding.  This is define in the column meta data: \n",
    "    *This is the occurred incident type created in the CAD program.  A code signifies a traffic stop (T), suspicious vehicle stop (1196), pedestrian stop (1194) and bicycle stop (1194B).*\n",
    "    \n",
    "    \n",
    "Recall the description:\n",
    "\n",
    "### Stop Data\n",
    "<img src=\"stops_desc.png\" width=800px />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "--- \n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "# EDA \n",
    "\n",
    "Now that we have loaded our various data files.  Let's try to understand a bit more about the data by examining properties of individual fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on the Calls Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.651869Z",
     "start_time": "2018-02-01T05:19:42.634036Z"
    }
   },
   "outputs": [],
   "source": [
    "calls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/> \n",
    "\n",
    "### Checking that City and State are Redundant\n",
    "\n",
    "We notice that there are city and state columns.  Since this is supposed to be data for the city of Berkeley these columns appear to be redundant.  Let's quickly compute the number of occurences of unique values for these two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.673246Z",
     "start_time": "2018-02-01T05:19:42.653818Z"
    }
   },
   "outputs": [],
   "source": [
    "calls.groupby([\"City\", \"State\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "### Are Case Numbers unique?\n",
    "\n",
    "Case numbers are probably used internally to track individual cases and my reference other data we don't have access to.  However, it is possible that multiple calls could be associated with the same case.  Let's see if the case numbers are all unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:42.680943Z",
     "start_time": "2018-02-01T05:19:42.675726Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"There are\", len(calls['CASENO'].unique()), \"unique case numbers.\")\n",
    "print(\"There are\", len(calls), \"calls in the table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are case numbers assigned consecutively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:43.190614Z",
     "start_time": "2018-02-01T05:19:42.682631Z"
    }
   },
   "outputs": [],
   "source": [
    "calls['CASENO'].sort_values().reset_index(drop=True).plot()\n",
    "plt.xlabel(\"Ordered Location in Data\")\n",
    "plt.ylabel(\"Case Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to use interactive plotting tools so I can hover the mouse over the plot and read the values.  The cufflinks library adds plotly support to Pandas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:44.807888Z",
     "start_time": "2018-02-01T05:19:43.192889Z"
    }
   },
   "outputs": [],
   "source": [
    "calls['CASENO'].sort_values().reset_index(drop=True).iplot(\n",
    "    layout = dict(yaxis=dict(title=\"Case Number\", hoverformat=\"d\"), \n",
    "                  xaxis=dict(title=\"Ordered Location in Data\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the distribution of case numbers shows a similar pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:46.254494Z",
     "start_time": "2018-02-01T05:19:44.810144Z"
    }
   },
   "outputs": [],
   "source": [
    "calls['CASENO'].iplot(kind=\"hist\",bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What might we be observing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible explanation is that case numbers were assigned consecutively and then sampled uniformly at different rates for two different periods.  We will be able to understand this better by looking at the dates on the cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "## Examining the Date\n",
    "\n",
    "Given the weird behavior with the case numbers let's dig into the date in which events were recorded.  Notice in this data we have several pieces of date/time information (this is not uncommon):\n",
    "1. **`EVENTDT`**: This contains the date the event took place.  While it has time information the time appears to be `00:00:00`.  \n",
    "1. **`EVENTTM`**: This contains the time at which the event took place.\n",
    "1. **`InDbDate`**: This appears to be the date at which the data was entered in the database.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:46.277023Z",
     "start_time": "2018-02-01T05:19:46.256803Z"
    }
   },
   "outputs": [],
   "source": [
    "calls.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Pandas loads more complex fields like dates it will often load them as strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:46.283631Z",
     "start_time": "2018-02-01T05:19:46.278896Z"
    }
   },
   "outputs": [],
   "source": [
    "calls[\"EVENTDT\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to convert these to dates.  Pandas has a fairly sophisticated function `pd.to_datetime` which is capable of guessing reasonable conversions of dates to date objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:47.349566Z",
     "start_time": "2018-02-01T05:19:46.285684Z"
    }
   },
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(calls[\"EVENTDT\"])\n",
    "dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the translations worked by looking at a few dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:47.360521Z",
     "start_time": "2018-02-01T05:19:47.351671Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(dict(transformed=dates, original=calls[\"EVENTDT\"])).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract the time field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:47.609343Z",
     "start_time": "2018-02-01T05:19:47.362310Z"
    }
   },
   "outputs": [],
   "source": [
    "times = pd.to_datetime(calls[\"EVENTTM\"]).dt.time\n",
    "times.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine the correct date and correct time field we use the built-in python datetime combine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:48.054631Z",
     "start_time": "2018-02-01T05:19:47.611439Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestamps = pd.to_datetime(pd.concat([dates, times], axis=1).apply(\n",
    "    lambda r: datetime.combine(r['EVENTDT'], r['EVENTTM']), axis=1))\n",
    "timestamps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now updated calls to contain this additional informations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:48.074375Z",
     "start_time": "2018-02-01T05:19:48.056802Z"
    }
   },
   "outputs": [],
   "source": [
    "calls['timestamp'] = timestamps\n",
    "calls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What time range does the data represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:48.081887Z",
     "start_time": "2018-02-01T05:19:48.076267Z"
    }
   },
   "outputs": [],
   "source": [
    "calls['timestamp'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:48.087913Z",
     "start_time": "2018-02-01T05:19:48.083857Z"
    }
   },
   "outputs": [],
   "source": [
    "calls['timestamp'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "### Back to the Case Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:50.699193Z",
     "start_time": "2018-02-01T05:19:48.089509Z"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    calls\n",
    "        .sort_values('timestamp')\n",
    "        .iplot(\n",
    "            mode='markers', size=5,\n",
    "            x='timestamp', y='CASENO',\n",
    "            layout = dict(yaxis=dict(title=\"Case Number\", hoverformat=\"d\"), \n",
    "                  xaxis=dict(title=\"Date of Call\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps there are multiple different record books with different numbering schemes?  This might be something worth investigating further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Negative Result...\n",
    "\n",
    "In the above analysis of case numbers all we can really conclude is that we don't understand how this variable was created.  The patterns in the values suggest that there may be some sampling and it is possibly worth contacting the data provider to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "### Are there any other interesting temporal patterns\n",
    "\n",
    "Do more calls occur on a particular day of the week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:52.094097Z",
     "start_time": "2018-02-01T05:19:50.701830Z"
    }
   },
   "outputs": [],
   "source": [
    "dow = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n",
    "calls.groupby('Day')['CASENO'].count()[dow].iplot(kind='bar', yTitle=\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### How about temporal patterns within a day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:52.109657Z",
     "start_time": "2018-02-01T05:19:52.096605Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = calls['timestamp']\n",
    "minute_of_day = ts.dt.hour * 60 + ts.dt.minute\n",
    "hour_of_day = minute_of_day / 60.\n",
    "\n",
    "calls['minute_of_day'] = minute_of_day\n",
    "calls['hour_of_day'] = hour_of_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:52.273855Z",
     "start_time": "2018-02-01T05:19:52.111844Z"
    }
   },
   "outputs": [],
   "source": [
    "py.iplot(ff.create_distplot([hour_of_day],group_labels=[\"Hour\"],bin_size=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot we see the standard pattern of limited activity early in the morning around here 6:00AM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "### Smoothing Parameters\n",
    "\n",
    "In the above plot we see a smoothed curve approximating the histogram.  This is an example of a kernel density estimator (KDE).  The KDE, like the histogram, has a parameter that determines it's smoothness.  Many packages (Plotly and Seaborn) use a boostrap like procedure to choose the best value.  \n",
    "\n",
    "To understand how this parameter works we will use seaborn which let's you also set the parameter manually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:52.434036Z",
     "start_time": "2018-02-01T05:19:52.275723Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(hour_of_day, rug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the above plot that the interpolation tries to follow the histogram to closely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Analysis\n",
    "\n",
    "To better understand the time of day a report occurs we could stratify the analysis by the day of the week.  To do this we will use box plots.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:52.439066Z",
     "start_time": "2018-02-01T05:19:52.436245Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using Pandas built in box plot\n",
    "# calls['hour_of_day'] = minute_in_day\n",
    "# calls.boxplot('hour_of_day', by='Day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:52.481801Z",
     "start_time": "2018-02-01T05:19:52.441045Z"
    }
   },
   "outputs": [],
   "source": [
    "groups = calls.groupby(\"Day\")\n",
    "boxes = {i: go.Box(y=df[\"hour_of_day\"], name=i) for (i, df) in groups}\n",
    "py.iplot([boxes[d] for d in dow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:52.859087Z",
     "start_time": "2018-02-01T05:19:52.483944Z"
    }
   },
   "outputs": [],
   "source": [
    "py.iplot(ff.create_violin(calls, data_header='hour_of_day', group_header=\"Day\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no very clear patterns here.  It is possible Fridays might have crimes shifted slightly more to the evening.  In general the time a crime is reported seems fairly consistent throughout the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "### Examining the Event\n",
    "\n",
    "We also have data about the different kinds of crimes being reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:52.880461Z",
     "start_time": "2018-02-01T05:19:52.861112Z"
    }
   },
   "outputs": [],
   "source": [
    "calls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Offense Field\n",
    "\n",
    "The Offense field appears to contain the specific crime being reported.  As nominal data we might want to see a summary constructed by computing counts of each offense type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:54.347914Z",
     "start_time": "2018-02-01T05:19:52.882284Z"
    }
   },
   "outputs": [],
   "source": [
    "calls['OFFENSE'].value_counts(dropna=False).iplot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Car burglary and misdemeanor theft seem to be the most common crimes with many other types of crimes occurring rarely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/>\n",
    "\n",
    "### CVLEGEND\n",
    "\n",
    "The CVLEGEND filed provides the broad category of crime and is a good mechanism to group potentially similar crimes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:55.455967Z",
     "start_time": "2018-02-01T05:19:54.349918Z"
    }
   },
   "outputs": [],
   "source": [
    "calls['CVLEGEND'].value_counts(dropna=False).iplot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we group by the crime time we see that **larceny** becomes the dominant category.  Larceny is essentially stealing -- taking someone else stuff without force."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Analysis of Time of Day by CVLEGEND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the crime time periods broken down by crime type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:55.509139Z",
     "start_time": "2018-02-01T05:19:55.458365Z"
    }
   },
   "outputs": [],
   "source": [
    "boxes = [(len(df), go.Box(y=df[\"hour_of_day\"], name=i)) for (i, df) in calls.groupby(\"CVLEGEND\")]\n",
    "py.iplot([r[1] for r in sorted(boxes, key=lambda x:x[0], reverse=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:55.596859Z",
     "start_time": "2018-02-01T05:19:55.512154Z"
    }
   },
   "outputs": [],
   "source": [
    "py.iplot(ff.create_distplot([\n",
    "    calls[calls['CVLEGEND'] == \"NOISE VIOLATION\"]['hour_of_day'],\n",
    "    calls[calls['CVLEGEND'] == \"DRUG VIOLATION\"]['hour_of_day'],\n",
    "    calls[calls['CVLEGEND'] == \"LIQUOR LAW VIOLATION\"]['hour_of_day'],\n",
    "    calls[calls['CVLEGEND'] == \"FRAUD\"]['hour_of_day']\n",
    "],\n",
    "    group_labels=[\"Noise Violation\", \"Drug Violation\", \"Liquor Violation\", \"Fraud\"], \n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Location information\n",
    "\n",
    "Let's examine the geographic data (latitude and longitude).  Recall that we had some missing values.  Let's look at the behavior of these missing values according to crime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:56.890301Z",
     "start_time": "2018-02-01T05:19:55.601240Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_lat_lon = calls[calls[['Lat', 'Lon']].isnull().any(axis=1)]\n",
    "missing_lat_lon['CVLEGEND'].value_counts().iplot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear bias towards drug violations that is not present in the original data.  Therefore we should be careful when dropping missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might further normalize the analysis by the frequency to find which type of crime has the highest proportion of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:58.134325Z",
     "start_time": "2018-02-01T05:19:56.892522Z"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    missing_lat_lon['CVLEGEND'].value_counts() / calls['CVLEGEND'].value_counts()\n",
    ").sort_values(ascending=False).iplot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/> \n",
    "\n",
    "### Day of Year and Missing Values:\n",
    "\n",
    "We might also want to examine the time of day in which latitude and longitude are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:58.281161Z",
     "start_time": "2018-02-01T05:19:58.136903Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = calls['timestamp']\n",
    "\n",
    "py.iplot(ff.create_distplot([calls['timestamp'].dt.dayofyear, \n",
    "                             missing_lat_lon['timestamp'].dt.dayofyear],\n",
    "                            group_labels=[\"all data\", \"missing lat-lon\"],bin_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:59.829356Z",
     "start_time": "2018-02-01T05:19:58.282941Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade folium\n",
    "import folium\n",
    "print(folium.__version__, \"should be at least 0.3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:19:59.969841Z",
     "start_time": "2018-02-01T05:19:59.831552Z"
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "import folium.plugins # The Folium Javascript Map Library\n",
    "\n",
    "SF_COORDINATES = (37.87, -122.28)\n",
    "sf_map = folium.Map(location=SF_COORDINATES, zoom_start=13)\n",
    "locs = calls[['Lat', 'Lon']].astype('float').dropna().as_matrix()\n",
    "heatmap = folium.plugins.HeatMap(locs.tolist(), radius = 10)\n",
    "sf_map.add_child(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Is campus really the safest place to be?\n",
    "1. Why are all the calls located on the street and at often at intersections?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T05:20:18.148189Z",
     "start_time": "2018-02-01T05:19:59.972311Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster = folium.plugins.MarkerCluster()\n",
    "for _, r in calls[['Lat', 'Lon', 'CVLEGEND']].tail(1000).dropna().iterrows():\n",
    "    cluster.add_child(\n",
    "        folium.Marker([float(r[\"Lat\"]), float(r[\"Lon\"])], popup=r['CVLEGEND']))\n",
    "    \n",
    "sf_map = folium.Map(location=SF_COORDINATES, zoom_start=13)\n",
    "sf_map.add_child(cluster)\n",
    "sf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
