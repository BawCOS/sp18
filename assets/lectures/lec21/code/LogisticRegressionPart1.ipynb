{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.469577Z",
     "start_time": "2018-04-02T16:07:15.663122Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=False, world_readable=True, theme='ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This is the notebook accompanies the lecture on Logistic Regression.\n",
    "\n",
    "Notebook created by [Joseph E. Gonzalez](https://eecs.berkeley.edu/~jegonzal) for DS100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data\n",
    "\n",
    "For this lecture we will use the Wisconsin Breast Cancer Dataset which we can obtain from [scikit learn](http://scikit-learn.org/stable/datasets/index.html#breast-cancer-wisconsin-diagnostic-database).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.590723Z",
     "start_time": "2018-04-02T16:07:17.472304Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "data_dict = sklearn.datasets.load_breast_cancer()\n",
    "data = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "# Target data_dict['target'] = 0 is malignant 1 is benign\n",
    "data['malignant'] = (data_dict['target'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.599437Z",
     "start_time": "2018-04-02T16:07:17.593220Z"
    }
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.755067Z",
     "start_time": "2018-04-02T16:07:17.601012Z"
    }
   },
   "outputs": [],
   "source": [
    "points = go.Scatter(x=data['mean radius'], y = 1.*data['malignant'], mode=\"markers\")\n",
    "layout = dict(xaxis=dict(title=\"Mean Radius\"),yaxis=dict(title=\"Malignant\"))\n",
    "py.iplot(go.Figure(data=[points], layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a clear example of over-plotting.  We can improve the above plot by jittering the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.773256Z",
     "start_time": "2018-04-02T16:07:17.757563Z"
    }
   },
   "outputs": [],
   "source": [
    "jitter_y = data['malignant'] + 0.1 * np.random.rand(data['malignant'].size) -0.05\n",
    "points = go.Scatter(x=data['mean radius'], y = jitter_y, \n",
    "                    mode=\"markers\", \n",
    "                    marker=dict(opacity=0.5))\n",
    "py.iplot(go.Figure(data=[points], layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a better way to visualize the data is using stacked histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.896849Z",
     "start_time": "2018-04-02T16:07:17.775139Z"
    }
   },
   "outputs": [],
   "source": [
    "py.iplot(ff.create_distplot(\n",
    "    [data.loc[~data['malignant'], 'mean radius'],\n",
    "     data.loc[data['malignant'], 'mean radius']], \n",
    "    group_labels=[\"Benign\",\"Malignant\"],\n",
    "    bin_size=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Looking at the above histograms could you describe a rule to predict whether or a cell is malignant?\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Regression\n",
    "\n",
    "_\"I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail.\"_ -  Abraham Maslow The Psychology of Science\n",
    "\n",
    "**Goal:** We would like to predict whether the tumor is malignant from the size of the tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always split your data into training and test groups.  \n",
    "\n",
    "## Preparing the data Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.924717Z",
     "start_time": "2018-04-02T16:07:17.899036Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_tr, data_te = train_test_split(data, test_size=0.25, random_state=42)\n",
    "print(\"Training Data Size: \", len(data_tr))\n",
    "print(\"Test Data Size: \", len(data_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define $X$ and $Y$ as variables containing the training features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.931358Z",
     "start_time": "2018-04-02T16:07:17.926492Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data_tr[['mean radius']].values\n",
    "Y = data_tr['malignant'].values.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a least squares regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.006048Z",
     "start_time": "2018-04-02T16:07:17.933377Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as linear_model\n",
    "\n",
    "least_squares_model = linear_model.LinearRegression()\n",
    "least_squares_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is our fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.026129Z",
     "start_time": "2018-04-02T16:07:18.007888Z"
    }
   },
   "outputs": [],
   "source": [
    "jitter_y = Y + 0.1*np.random.rand(len(Y)) - 0.05\n",
    "points = go.Scatter(name=\"Jittered Data\", \n",
    "                    x=np.squeeze(X), y = jitter_y, \n",
    "                    mode=\"markers\", marker=dict(opacity=0.5))\n",
    "X_plt = np.linspace(np.min(X), np.max(X), 10)\n",
    "model_line = go.Scatter(name=\"Least Squares\",\n",
    "    x=X_plt, y=least_squares_model.predict(X_plt[:,np.newaxis]), \n",
    "    mode=\"lines\", line=dict(color=\"orange\"))\n",
    "py.iplot([points, model_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "\n",
    "1. Are we happy with the fit?\n",
    "2. What is the meaning of predictions that are neither 0 or 1?\n",
    "3. Could we use this to make a decision?\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Root Mean Squared Error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.032417Z",
     "start_time": "2018-04-02T16:07:18.028104Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "print(\"Training RMSE:\", np.sqrt(mse(Y, least_squares_model.predict(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does that mean for this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "# Classification Error\n",
    "\n",
    "This is a classification problem so we probably want to measure how often we predict the correct value.  This is sometimes called the zero-one loss (or error):\n",
    "\n",
    "$$ \\large\n",
    "\\textbf{ZeroOneLoss} = \\frac{1}{n} \\sum_{i=1}^n \\textbf{I}\\left[ y_i \\neq f_\\theta(x) \\right]\n",
    "$$\n",
    "\n",
    "However to use the classification error we need to define a decision rule that maps $f_\\theta(x)$ to the $\\{0,1\\}$ classification values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "# Simple Decision Rule\n",
    "\n",
    "Suppose we instituted the following simple decision rule:\n",
    "\n",
    "$$\\Large\n",
    "\\text{If } f_\\theta(x) > 0.5  \\text{ predict 1 (malignant) else predict 0 (benign).}\n",
    "$$\n",
    "\n",
    "This simple **decision rule** is deciding that a tumor is malignant if our model predicts a values above 0.5 (closer to 1 than zero).\n",
    "\n",
    "In the following we plot the implication of these decisions on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.059302Z",
     "start_time": "2018-04-02T16:07:18.034361Z"
    }
   },
   "outputs": [],
   "source": [
    "jitter_y = Y + 0.1*np.random.rand(len(Y)) - 0.05\n",
    "ind_mal = least_squares_model.predict(X) > 0.5\n",
    "\n",
    "mal_points = go.Scatter(name=\"Classified as Malignant\", \n",
    "                    x=np.squeeze(X[ind_mal]), y = jitter_y[ind_mal], \n",
    "                    mode=\"markers\", marker=dict(opacity=0.5, color=\"red\"))\n",
    "ben_points = go.Scatter(name=\"Classified as Benign\", \n",
    "                    x=np.squeeze(X[~ind_mal]), y = jitter_y[~ind_mal], \n",
    "                    mode=\"markers\", marker=dict(opacity=0.5, color=\"blue\"))\n",
    "dec_boundary = (0.5 - least_squares_model.intercept_)/least_squares_model.coef_[0]\n",
    "dec_line = go.Scatter(name=\"Least Squares Decision Boundary\", \n",
    "                      x = [dec_boundary,dec_boundary], y=[-0.5,1.5], mode=\"lines\",\n",
    "                     line=dict(color=\"black\", dash=\"dot\"))\n",
    "py.iplot([mal_points, ben_points, model_line,dec_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute `ZeroOneLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.067202Z",
     "start_time": "2018-04-02T16:07:18.061723Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "print(\"Training Fraction incorrect:\", \n",
    "      zero_one_loss(Y, least_squares_model.predict(X) > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n",
    "\n",
    "1. Are we happy with this error level?\n",
    "1. What error would we get if we just guessed the label?\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guessing the Majority Class\n",
    "\n",
    "This is the simplest baseline we could imagine and one you should always compare against.  Let's start by asking what is the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.072065Z",
     "start_time": "2018-04-02T16:07:18.069177Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Fraction of Malignant Samples:\", np.mean(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore if we guess the majority class **benign** we would get what accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.078040Z",
     "start_time": "2018-04-02T16:07:18.074158Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can figure this out from the above number\n",
    "print(\"Guess Majority:\",  zero_one_loss(Y, np.zeros(len(Y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is standard example of a common problem in classification (and perhaps modern society): **class imbalance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance\n",
    "\n",
    "\n",
    "Class imbalance is when a disproportionate fraction of the samples are in one class (in this case benign).  In extreme cases (e.g., fraud detection) only tiny fraction of the training data may contain examples in particular class.  In these settings we can achieve very high-accuracies by always predicting the frequent class without learning a good classifier for the rare classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Addressing Class Imbalance\n",
    "\n",
    "There are many techniques for managing class imbalance here are a few:\n",
    "\n",
    "1. Re-sample data to reduce or eliminate the class imbalance.\n",
    "2. Try learning algorithm that are a little more robust to class imbalance (e.g., decisions trees).\n",
    "3. Adjust the loss function to put a larger penalty on the smaller class.\n",
    "\n",
    "In this example the class imbalance is not that extreme so we will continue without re-sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "# Cross Validation of Zero-One Error\n",
    "\n",
    "In the following we compute the cross validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.096792Z",
     "start_time": "2018-04-02T16:07:18.080117Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(3,shuffle=True, random_state=42)\n",
    "linreg_errors = []\n",
    "models = []\n",
    "\n",
    "for tr_ind, te_ind in kfold.split(X):\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(X[tr_ind,], Y[tr_ind])\n",
    "    models.append(model)\n",
    "    linreg_errors.append(zero_one_loss(Y[te_ind], model.predict(X[te_ind,]) > 0.5))\n",
    "    \n",
    "print(\"Min Validation Error:   \", np.min(linreg_errors))\n",
    "print(\"Median Validation Error:\", np.median(linreg_errors))\n",
    "print(\"Max Validation Error:   \", np.max(linreg_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize all the models and their decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.126257Z",
     "start_time": "2018-04-02T16:07:18.099160Z"
    }
   },
   "outputs": [],
   "source": [
    "dec_lines = [\n",
    "    go.Scatter(name=\"Decision Boundary\", \n",
    "               x = [(0.5 - m.intercept_)/m.coef_[0]]*2, \n",
    "               y=[-0.5,1.5], mode=\"lines\",\n",
    "               line=dict(dash=\"dot\"))\n",
    "    for m in models]\n",
    "\n",
    "X_plt = np.linspace(np.min(X), np.max(X), 10)\n",
    "model_lines = [\n",
    "    go.Scatter(name=\"Least Squares \" + str(zero_one_loss(Y, m.predict(X) > 0.5)),\n",
    "               x=X_plt, y=m.predict(np.array([X_plt]).T), \n",
    "               mode=\"lines\")\n",
    "    for m in models]\n",
    "py.iplot([points] + model_lines + dec_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Can we think of the line as a _\"probability\"_?\n",
    "\n",
    "\n",
    "Not really.  Probabilities are constrained between 0 and 1.   How could we learn a model that captures this probabilistic interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Could we just truncate the line?\n",
    "\n",
    "Maybe we can define the probability as:\n",
    "\n",
    "$$ \\large\n",
    "p_i = \\min\\left(\\max \\left( x^T \\theta , 0 \\right), 1\\right)\n",
    "$$\n",
    "\n",
    "this would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.131948Z",
     "start_time": "2018-04-02T16:07:18.128385Z"
    }
   },
   "outputs": [],
   "source": [
    "def bound01(z):\n",
    "    u = np.where(z > 1, 1, z)\n",
    "    return np.where(u < 0, 0, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.155130Z",
     "start_time": "2018-04-02T16:07:18.134002Z"
    }
   },
   "outputs": [],
   "source": [
    "X_plt = np.linspace(np.min(X), np.max(X), 100)\n",
    "p_line = go.Scatter(name=\"Truncated Least Squares\",\n",
    "    x=X_plt, y=bound01(least_squares_model.predict(np.array([X_plt]).T)), \n",
    "    mode=\"lines\", line=dict(color=\"green\", width=8))\n",
    "py.iplot([mal_points, ben_points, model_line, p_line, dec_line], filename=\"lr-06\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far least squares regression seems pretty reasonable and we can \"force\" the predicted values to be bounded between 0 and 1.  \n",
    "\n",
    "**Can we interpret the truncated values as probabilities?** \n",
    "\n",
    "Perhaps, but it would depend on how the model is estimated (more on this soon).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# An Issue with Extreme Points \n",
    "\n",
    "It seems like large tumor sizes are indicative of malignant tumors.  Suppose we observed a very large malignant tumor that is 100mm in mean radius.  What would this do to our model?\n",
    "\n",
    "\n",
    "Let's add an extra data point and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.165906Z",
     "start_time": "2018-04-02T16:07:18.157445Z"
    }
   },
   "outputs": [],
   "source": [
    "X_ex = np.vstack([X, [100]])\n",
    "Y_ex = np.hstack([Y, 1.])\n",
    "least_squares_model_ex = linear_model.LinearRegression()\n",
    "least_squares_model_ex.fit(X_ex, Y_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:08:52.741027Z",
     "start_time": "2018-04-02T16:08:52.706346Z"
    }
   },
   "outputs": [],
   "source": [
    "X_plt = np.linspace(np.min(X)-5, np.max(X)+5, 100)\n",
    "\n",
    "extreme_point = go.Scatter(\n",
    "    name=\"Extreme Point\", x=[100], y=[1], mode=\"markers\", \n",
    "    marker=dict(color=\"green\", size=10))\n",
    "model_line.line.color = \"gray\"\n",
    "model_line_ex = go.Scatter(name=\"New Least Squares\",\n",
    "    x=X_plt, y=least_squares_model_ex.predict(np.array([X_plt]).T), \n",
    "    mode=\"lines\", line=dict(color=\"orange\"))\n",
    "\n",
    "dec_line.line.color = \"gray\"\n",
    "\n",
    "dec_boundary_ex = (0.5 - least_squares_model_ex.intercept_)/least_squares_model_ex.coef_[0]\n",
    "dec_line_ex = go.Scatter(\n",
    "    name=\"Decision Boundary\", \n",
    "    x = [dec_boundary_ex, dec_boundary_ex], y=[-0.5,1.5], mode=\"lines\",\n",
    "    line=dict(color=\"black\", dash=\"dash\"))\n",
    "\n",
    "\n",
    "\n",
    "py.iplot([mal_points, ben_points,model_line, model_line_ex, dec_line, dec_line_ex, extreme_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing the resulting RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:09:16.748183Z",
     "start_time": "2018-04-02T16:09:16.743636Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Before:\", \n",
    "      zero_one_loss(Y_ex, least_squares_model.predict(X_ex) > 0.5))\n",
    "print(\"After:\", \n",
    "      zero_one_loss(Y_ex, least_squares_model_ex.predict(X_ex) > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
